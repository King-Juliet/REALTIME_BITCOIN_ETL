{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec72c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#batch process data by hardcoding block height of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9c5aa3a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-01 16:12:42,950 [INFO] ðŸ”„ Extracting block 800000\n",
      "2025-10-01 16:12:44,751 [INFO] âœ… Got block hash: 00000000000000000002a7c4c1e48d76c5a37902165a270156b7a8d72728a054\n",
      "2025-10-01 16:12:48,433 [INFO] ðŸ“¦ Found 25 transactions in block 800000 (first page)\n",
      "2025-10-01 16:13:31,105 [INFO] âœ… Extracted 25 detailed transactions\n",
      "2025-10-01 16:13:31,121 [INFO] ðŸ”„ Transforming data...\n",
      "2025-10-01 16:13:31,137 [INFO] âœ… Transform complete: 25 txs, 30 inputs, 68 outputs, 65 addresses\n",
      "2025-10-01 16:13:31,137 [INFO] ðŸ”„ Loading data into PostgreSQL...\n",
      "2025-10-01 16:13:31,224 [INFO] Inserting 25 transactions...\n",
      "2025-10-01 16:13:31,236 [INFO] Inserting 30 inputs...\n",
      "2025-10-01 16:13:31,251 [INFO] Inserting 68 outputs...\n",
      "2025-10-01 16:13:31,274 [INFO] Upserting 65 addresses...\n",
      "2025-10-01 16:13:31,303 [INFO] âœ… Load completed successfully.\n",
      "2025-10-01 16:13:31,305 [INFO] ðŸŽ‰ ETL completed with UTXO + address-level data (with first_seen/last_seen).\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import psycopg2\n",
    "import pandas as pd\n",
    "import logging\n",
    "import sys\n",
    "import traceback\n",
    "\n",
    "# ------------------------------\n",
    "# CONFIGURATION\n",
    "# ------------------------------\n",
    "BLOCKSTREAM_API = \"https://blockstream.info/api\"\n",
    "DB_HOST = \"localhost\"\n",
    "DB_PORT = \"5432\"\n",
    "DB_NAME = \"bitcoin_db\"\n",
    "DB_USER = \"postgres\"\n",
    "DB_PASS = \"chibuoke3456\"\n",
    "\n",
    "# ------------------------------\n",
    "# LOGGING SETUP\n",
    "# ------------------------------\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s [%(levelname)s] %(message)s\",\n",
    "    handlers=[\n",
    "        logging.StreamHandler(sys.stdout),\n",
    "        logging.FileHandler(\"etl_bitcoin.log\", mode=\"a\")\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "# ------------------------------\n",
    "# EXTRACT\n",
    "# ------------------------------\n",
    "def extract_block(block_height: int):\n",
    "    try:\n",
    "        logger.info(f\"ðŸ”„ Extracting block {block_height}\")\n",
    "\n",
    "        block_hash = requests.get(f\"{BLOCKSTREAM_API}/block-height/{block_height}\").text.strip()\n",
    "        logger.info(f\"âœ… Got block hash: {block_hash}\")\n",
    "\n",
    "        block_data = requests.get(f\"{BLOCKSTREAM_API}/block/{block_hash}\").json()\n",
    "\n",
    "        txs = requests.get(f\"{BLOCKSTREAM_API}/block/{block_hash}/txs\").json()\n",
    "        logger.info(f\"ðŸ“¦ Found {len(txs)} transactions in block {block_height} (first page)\")\n",
    "\n",
    "        detailed_txs = []\n",
    "        for tx in txs:\n",
    "            txid = tx[\"txid\"]\n",
    "            try:\n",
    "                detailed_tx = requests.get(f\"{BLOCKSTREAM_API}/tx/{txid}\").json()\n",
    "                detailed_txs.append(detailed_tx)\n",
    "            except Exception as e:\n",
    "                logger.error(f\"âš ï¸ Failed to fetch tx {txid}: {e}\")\n",
    "                logger.error(traceback.format_exc())\n",
    "\n",
    "        logger.info(f\"âœ… Extracted {len(detailed_txs)} detailed transactions\")\n",
    "        return block_data, detailed_txs\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"âŒ Extract step failed: {e}\")\n",
    "        logger.error(traceback.format_exc())\n",
    "        raise\n",
    "\n",
    "\n",
    "# ------------------------------\n",
    "# TRANSFORM\n",
    "# ------------------------------\n",
    "def transform_block(block_data, detailed_txs):\n",
    "    try:\n",
    "        logger.info(\"ðŸ”„ Transforming data...\")\n",
    "\n",
    "        block_df = pd.DataFrame([{\n",
    "            \"block_id\": block_data[\"id\"],\n",
    "            \"height\": block_data[\"height\"],\n",
    "            \"timestamp\": pd.to_datetime(block_data[\"timestamp\"], unit=\"s\"),\n",
    "            \"tx_count\": block_data[\"tx_count\"],\n",
    "            \"size\": block_data[\"size\"],\n",
    "            \"weight\": block_data[\"weight\"]\n",
    "        }])\n",
    "\n",
    "        tx_list, input_list, output_list = [], [], []\n",
    "        address_dict = {}\n",
    "\n",
    "        for tx in detailed_txs:\n",
    "            tx_time = pd.to_datetime(tx.get(\"status\", {}).get(\"block_time\"), unit=\"s\")\n",
    "\n",
    "            tx_list.append({\n",
    "                \"txid\": tx[\"txid\"],\n",
    "                \"block_id\": block_data[\"id\"],\n",
    "                \"fee\": tx.get(\"fee\"),\n",
    "                \"size\": tx.get(\"size\"),\n",
    "                \"weight\": tx.get(\"weight\"),\n",
    "                \"version\": tx.get(\"version\"),\n",
    "                \"locktime\": tx.get(\"locktime\")\n",
    "            })\n",
    "\n",
    "            # Inputs\n",
    "            for vin in tx.get(\"vin\", []):\n",
    "                prevout = vin.get(\"prevout\") or {}\n",
    "                input_list.append({\n",
    "                    \"txid\": tx[\"txid\"],\n",
    "                    \"prev_txid\": vin.get(\"txid\"),\n",
    "                    \"prev_index\": vin.get(\"vout\"),\n",
    "                    \"input_address\": prevout.get(\"scriptpubkey_address\"),\n",
    "                    \"input_value\": prevout.get(\"value\")\n",
    "                })\n",
    "\n",
    "                addr = prevout.get(\"scriptpubkey_address\")\n",
    "                if addr:\n",
    "                    address_dict.setdefault(addr, {\n",
    "                        \"total_received\": 0, \"total_sent\": 0,\n",
    "                        \"first_seen\": tx_time, \"last_seen\": tx_time\n",
    "                    })\n",
    "                    address_dict[addr][\"total_sent\"] += prevout.get(\"value\", 0)\n",
    "                    address_dict[addr][\"last_seen\"] = tx_time\n",
    "\n",
    "            # Outputs\n",
    "            for idx, vout in enumerate(tx.get(\"vout\", [])):\n",
    "                output_list.append({\n",
    "                    \"txid\": tx[\"txid\"],\n",
    "                    \"vout_index\": idx,\n",
    "                    \"output_address\": vout.get(\"scriptpubkey_address\"),\n",
    "                    \"output_value\": vout.get(\"value\"),\n",
    "                    \"spent\": vout.get(\"spent\", False)\n",
    "                })\n",
    "\n",
    "                addr = vout.get(\"scriptpubkey_address\")\n",
    "                if addr:\n",
    "                    address_dict.setdefault(addr, {\n",
    "                        \"total_received\": 0, \"total_sent\": 0,\n",
    "                        \"first_seen\": tx_time, \"last_seen\": tx_time\n",
    "                    })\n",
    "                    address_dict[addr][\"total_received\"] += vout.get(\"value\", 0)\n",
    "                    if address_dict[addr][\"first_seen\"] is None:\n",
    "                        address_dict[addr][\"first_seen\"] = tx_time\n",
    "                    address_dict[addr][\"last_seen\"] = tx_time\n",
    "\n",
    "        tx_df = pd.DataFrame(tx_list)\n",
    "        inputs_df = pd.DataFrame(input_list)\n",
    "        outputs_df = pd.DataFrame(output_list)\n",
    "\n",
    "        addr_list = []\n",
    "        for addr, stats in address_dict.items():\n",
    "            balance = stats[\"total_received\"] - stats[\"total_sent\"]\n",
    "            addr_list.append({\n",
    "                \"address\": addr,\n",
    "                \"total_received\": stats[\"total_received\"],\n",
    "                \"total_sent\": stats[\"total_sent\"],\n",
    "                \"balance\": balance,\n",
    "                \"first_seen\": stats[\"first_seen\"],\n",
    "                \"last_seen\": stats[\"last_seen\"]\n",
    "            })\n",
    "        address_df = pd.DataFrame(addr_list)\n",
    "\n",
    "        logger.info(f\"âœ… Transform complete: {len(tx_df)} txs, {len(inputs_df)} inputs, {len(outputs_df)} outputs, {len(address_df)} addresses\")\n",
    "        return block_df, tx_df, inputs_df, outputs_df, address_df\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"âŒ Transform step failed: {e}\")\n",
    "        logger.error(traceback.format_exc())\n",
    "        raise\n",
    "\n",
    "\n",
    "# ------------------------------\n",
    "# LOAD\n",
    "# ------------------------------\n",
    "def load_to_postgres(block_df, tx_df, inputs_df, outputs_df, address_df):\n",
    "    try:\n",
    "        logger.info(\"ðŸ”„ Loading data into PostgreSQL...\")\n",
    "\n",
    "        conn = psycopg2.connect(\n",
    "            host=DB_HOST, port=DB_PORT,\n",
    "            dbname=DB_NAME, user=DB_USER, password=DB_PASS\n",
    "        )\n",
    "        cursor = conn.cursor()\n",
    "\n",
    "        # Create tables\n",
    "        cursor.execute(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS bitcoin_blocks (\n",
    "            block_id TEXT PRIMARY KEY,\n",
    "            height NUMERIC,\n",
    "            timestamp TIMESTAMP,\n",
    "            tx_count NUMERIC,\n",
    "            size BIGINT,\n",
    "            weight BIGINT\n",
    "        );\n",
    "        \"\"\")\n",
    "        cursor.execute(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS bitcoin_transactions (\n",
    "            txid TEXT PRIMARY KEY,\n",
    "            block_id TEXT,\n",
    "            fee NUMERIC,\n",
    "            size BIGINT,\n",
    "            weight BIGINT,\n",
    "            version NUMERIC,\n",
    "            locktime BIGINT,\n",
    "            FOREIGN KEY(block_id) REFERENCES bitcoin_blocks(block_id)\n",
    "        );\n",
    "        \"\"\")\n",
    "        cursor.execute(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS bitcoin_inputs (\n",
    "            id SERIAL PRIMARY KEY,\n",
    "            txid TEXT,\n",
    "            prev_txid TEXT,\n",
    "            prev_index NUMERIC,\n",
    "            input_address TEXT,\n",
    "            input_value NUMERIC\n",
    "        );\n",
    "        \"\"\")\n",
    "        cursor.execute(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS bitcoin_outputs (\n",
    "            id SERIAL PRIMARY KEY,\n",
    "            txid TEXT,\n",
    "            vout_index NUMERIC,\n",
    "            output_address TEXT,\n",
    "            output_value NUMERIC,\n",
    "            spent BOOLEAN\n",
    "        );\n",
    "        \"\"\")\n",
    "        cursor.execute(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS bitcoin_addresses (\n",
    "            address TEXT PRIMARY KEY,\n",
    "            total_received NUMERIC,\n",
    "            total_sent NUMERIC,\n",
    "            balance NUMERIC,\n",
    "            first_seen TIMESTAMP,\n",
    "            last_seen TIMESTAMP\n",
    "        );\n",
    "        \"\"\")\n",
    "\n",
    "        # Insert block\n",
    "        for _, row in block_df.iterrows():\n",
    "            try:\n",
    "                cursor.execute(\"\"\"\n",
    "                INSERT INTO bitcoin_blocks (block_id, height, timestamp, tx_count, size, weight)\n",
    "                VALUES (%s, %s, %s, %s, %s, %s)\n",
    "                ON CONFLICT (block_id) DO NOTHING;\n",
    "                \"\"\", (row.block_id, row.height, row.timestamp, row.tx_count, row.size, row.weight))\n",
    "            except Exception as e:\n",
    "                logger.error(f\"âŒ Failed inserting block: {e}\")\n",
    "                logger.error(traceback.format_exc())\n",
    "\n",
    "        # Insert transactions\n",
    "        logger.info(f\"Inserting {len(tx_df)} transactions...\")\n",
    "        for _, row in tx_df.iterrows():\n",
    "            try:\n",
    "                cursor.execute(\"\"\"\n",
    "                INSERT INTO bitcoin_transactions (txid, block_id, fee, size, weight, version, locktime)\n",
    "                VALUES (%s, %s, %s, %s, %s, %s, %s)\n",
    "                ON CONFLICT (txid) DO NOTHING;\n",
    "                \"\"\", (row.txid, row.block_id, row.fee, row.size, row.weight, row.version, row.locktime))\n",
    "            except Exception as e:\n",
    "                logger.error(f\"âŒ Failed inserting transaction {row.txid}: {e}\")\n",
    "                logger.error(traceback.format_exc())\n",
    "\n",
    "        # Insert inputs\n",
    "        logger.info(f\"Inserting {len(inputs_df)} inputs...\")\n",
    "        for _, row in inputs_df.iterrows():\n",
    "            try:\n",
    "                cursor.execute(\"\"\"\n",
    "                INSERT INTO bitcoin_inputs (txid, prev_txid, prev_index, input_address, input_value)\n",
    "                VALUES (%s, %s, %s, %s, %s)\n",
    "                \"\"\", (row.txid, row.prev_txid, row.prev_index, row.input_address, row.input_value))\n",
    "            except Exception as e:\n",
    "                logger.error(f\"âŒ Failed inserting input for tx {row.txid}: {e}\")\n",
    "                logger.error(traceback.format_exc())\n",
    "\n",
    "        # Insert outputs\n",
    "        logger.info(f\"Inserting {len(outputs_df)} outputs...\")\n",
    "        for _, row in outputs_df.iterrows():\n",
    "            try:\n",
    "                cursor.execute(\"\"\"\n",
    "                INSERT INTO bitcoin_outputs (txid, vout_index, output_address, output_value, spent)\n",
    "                VALUES (%s, %s, %s, %s, %s)\n",
    "                \"\"\", (row.txid, row.vout_index, row.output_address, row.output_value, row.spent))\n",
    "            except Exception as e:\n",
    "                logger.error(f\"âŒ Failed inserting output for tx {row.txid}: {e}\")\n",
    "                logger.error(traceback.format_exc())\n",
    "\n",
    "        # Insert/update addresses\n",
    "        logger.info(f\"Upserting {len(address_df)} addresses...\")\n",
    "        for _, row in address_df.iterrows():\n",
    "            try:\n",
    "                cursor.execute(\"\"\"\n",
    "                INSERT INTO bitcoin_addresses (address, total_received, total_sent, balance, first_seen, last_seen)\n",
    "                VALUES (%s, %s, %s, %s, %s, %s)\n",
    "                ON CONFLICT (address) \n",
    "                DO UPDATE SET total_received = bitcoin_addresses.total_received + EXCLUDED.total_received,\n",
    "                              total_sent = bitcoin_addresses.total_sent + EXCLUDED.total_sent,\n",
    "                              balance = bitcoin_addresses.balance + EXCLUDED.balance,\n",
    "                              last_seen = GREATEST(bitcoin_addresses.last_seen, EXCLUDED.last_seen);\n",
    "                \"\"\", (row.address, row.total_received, row.total_sent, row.balance, row.first_seen, row.last_seen))\n",
    "            except Exception as e:\n",
    "                logger.error(f\"âŒ Failed upserting address {row.address}: {e}\")\n",
    "                logger.error(traceback.format_exc())\n",
    "\n",
    "        conn.commit()\n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "        logger.info(\"âœ… Load completed successfully.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"âŒ Load step failed: {e}\")\n",
    "        logger.error(traceback.format_exc())\n",
    "        raise\n",
    "\n",
    "\n",
    "# ------------------------------\n",
    "# MAIN ETL\n",
    "# ------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        block_data, detailed_txs = extract_block(block_height=800000)\n",
    "        block_df, tx_df, inputs_df, outputs_df, address_df = transform_block(block_data, detailed_txs)\n",
    "        load_to_postgres(block_df, tx_df, inputs_df, outputs_df, address_df)\n",
    "        logger.info(\"ðŸŽ‰ ETL completed with UTXO + address-level data (with first_seen/last_seen).\")\n",
    "    except Exception as e:\n",
    "        logger.critical(f\"ðŸš¨ ETL pipeline crashed: {e}\")\n",
    "        logger.critical(traceback.format_exc())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a81760",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f027f264",
   "metadata": {},
   "outputs": [],
   "source": [
    "# streaming data based on a specified block height "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "10da1b31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-01 16:38:29,070 [INFO] ðŸš€ Starting fresh from configured block 80000\n",
      "2025-10-01 16:38:29,114 [INFO] âœ… Updated checkpoint: last_block_height = 80000\n",
      "2025-10-01 16:38:30,935 [INFO] ðŸ“¦ Processing block 80001\n",
      "2025-10-01 16:38:30,935 [INFO] ðŸ”„ Extracting block 80001\n",
      "2025-10-01 16:38:32,858 [INFO] âœ… Got block hash: 00000000000036312a44ab7711afa46f475913fbd9727cf508ed4af3bc933d16\n",
      "2025-10-01 16:38:36,587 [INFO] ðŸ“¦ Found 3 transactions in block 80001 (first page)\n",
      "2025-10-01 16:38:41,356 [INFO] âœ… Extracted 3 detailed transactions\n",
      "2025-10-01 16:38:41,356 [INFO] ðŸ”„ Transforming data...\n",
      "2025-10-01 16:38:41,371 [INFO] âœ… Transform complete: 3 txs, 31 inputs, 3 outputs, 3 addresses\n",
      "2025-10-01 16:38:41,372 [INFO] ðŸ”„ Loading data into PostgreSQL...\n",
      "2025-10-01 16:38:41,426 [INFO] Inserting 3 transactions...\n",
      "2025-10-01 16:38:41,426 [INFO] Inserting 31 inputs...\n",
      "2025-10-01 16:38:41,447 [INFO] Inserting 3 outputs...\n",
      "2025-10-01 16:38:41,450 [INFO] Upserting 3 addresses...\n",
      "2025-10-01 16:38:41,455 [INFO] âœ… Load completed successfully.\n",
      "2025-10-01 16:38:41,499 [INFO] âœ… Updated checkpoint: last_block_height = 80001\n",
      "2025-10-01 16:38:41,502 [INFO] ðŸ“¦ Processing block 80002\n",
      "2025-10-01 16:38:41,502 [INFO] ðŸ”„ Extracting block 80002\n",
      "2025-10-01 16:38:43,106 [INFO] âœ… Got block hash: 0000000000242548cf2ae995356d8c3e45103f04d17a6bb4e0e7d89ec45d8045\n",
      "2025-10-01 16:38:46,073 [INFO] ðŸ“¦ Found 2 transactions in block 80002 (first page)\n",
      "2025-10-01 16:38:49,338 [INFO] âœ… Extracted 2 detailed transactions\n",
      "2025-10-01 16:38:49,338 [INFO] ðŸ”„ Transforming data...\n",
      "2025-10-01 16:38:49,357 [INFO] âœ… Transform complete: 2 txs, 2 inputs, 2 outputs, 1 addresses\n",
      "2025-10-01 16:38:49,357 [INFO] ðŸ”„ Loading data into PostgreSQL...\n",
      "2025-10-01 16:38:49,543 [INFO] Inserting 2 transactions...\n",
      "2025-10-01 16:38:49,543 [INFO] Inserting 2 inputs...\n",
      "2025-10-01 16:38:49,543 [INFO] Inserting 2 outputs...\n",
      "2025-10-01 16:38:49,553 [INFO] Upserting 1 addresses...\n",
      "2025-10-01 16:38:49,566 [INFO] âœ… Load completed successfully.\n",
      "2025-10-01 16:38:49,612 [INFO] âœ… Updated checkpoint: last_block_height = 80002\n",
      "2025-10-01 16:38:49,615 [INFO] ðŸ“¦ Processing block 80003\n",
      "2025-10-01 16:38:49,615 [INFO] ðŸ”„ Extracting block 80003\n",
      "2025-10-01 16:38:51,245 [INFO] âœ… Got block hash: 000000000042a2cf1cddf23ee040f9ee162f84db7898efc9a03c181b50c2f2a7\n",
      "2025-10-01 16:38:54,803 [INFO] ðŸ“¦ Found 6 transactions in block 80003 (first page)\n",
      "2025-10-01 16:39:06,015 [INFO] âœ… Extracted 6 detailed transactions\n",
      "2025-10-01 16:39:06,017 [INFO] ðŸ”„ Transforming data...\n",
      "2025-10-01 16:39:06,023 [INFO] âœ… Transform complete: 6 txs, 8 inputs, 8 outputs, 10 addresses\n",
      "2025-10-01 16:39:06,024 [INFO] ðŸ”„ Loading data into PostgreSQL...\n",
      "2025-10-01 16:39:06,073 [INFO] Inserting 6 transactions...\n",
      "2025-10-01 16:39:06,078 [INFO] Inserting 8 inputs...\n",
      "2025-10-01 16:39:06,084 [INFO] Inserting 8 outputs...\n",
      "2025-10-01 16:39:06,089 [INFO] Upserting 10 addresses...\n",
      "2025-10-01 16:39:06,107 [INFO] âœ… Load completed successfully.\n",
      "2025-10-01 16:39:06,150 [INFO] âœ… Updated checkpoint: last_block_height = 80003\n",
      "2025-10-01 16:39:06,156 [INFO] ðŸ“¦ Processing block 80004\n",
      "2025-10-01 16:39:06,156 [INFO] ðŸ”„ Extracting block 80004\n",
      "2025-10-01 16:39:07,906 [INFO] âœ… Got block hash: 000000000016c5db2655e018494959db5ac9f595786747f9087107a642c95a98\n",
      "2025-10-01 16:39:11,661 [INFO] ðŸ“¦ Found 6 transactions in block 80004 (first page)\n",
      "2025-10-01 16:39:24,225 [INFO] âœ… Extracted 6 detailed transactions\n",
      "2025-10-01 16:39:24,228 [INFO] ðŸ”„ Transforming data...\n",
      "2025-10-01 16:39:24,248 [INFO] âœ… Transform complete: 6 txs, 9 inputs, 9 outputs, 10 addresses\n",
      "2025-10-01 16:39:24,252 [INFO] ðŸ”„ Loading data into PostgreSQL...\n",
      "2025-10-01 16:39:24,321 [INFO] Inserting 6 transactions...\n",
      "2025-10-01 16:39:24,327 [INFO] Inserting 9 inputs...\n",
      "2025-10-01 16:39:24,332 [INFO] Inserting 9 outputs...\n",
      "2025-10-01 16:39:24,338 [INFO] Upserting 10 addresses...\n",
      "2025-10-01 16:39:24,346 [INFO] âœ… Load completed successfully.\n",
      "2025-10-01 16:39:24,390 [INFO] âœ… Updated checkpoint: last_block_height = 80004\n",
      "2025-10-01 16:39:24,391 [INFO] ðŸ“¦ Processing block 80005\n",
      "2025-10-01 16:39:24,392 [INFO] ðŸ”„ Extracting block 80005\n",
      "2025-10-01 16:39:25,876 [INFO] âœ… Got block hash: 000000000042cbc35a9ef3d63e9847165d8c17321b58b819f7645b699c3e25b1\n",
      "2025-10-01 16:39:29,184 [INFO] ðŸ“¦ Found 2 transactions in block 80005 (first page)\n",
      "2025-10-01 16:39:32,543 [INFO] âœ… Extracted 2 detailed transactions\n",
      "2025-10-01 16:39:32,545 [INFO] ðŸ”„ Transforming data...\n",
      "2025-10-01 16:39:32,559 [INFO] âœ… Transform complete: 2 txs, 2 inputs, 2 outputs, 1 addresses\n",
      "2025-10-01 16:39:32,561 [INFO] ðŸ”„ Loading data into PostgreSQL...\n",
      "2025-10-01 16:39:32,745 [INFO] Inserting 2 transactions...\n",
      "2025-10-01 16:39:32,750 [INFO] Inserting 2 inputs...\n",
      "2025-10-01 16:39:32,753 [INFO] Inserting 2 outputs...\n",
      "2025-10-01 16:39:32,757 [INFO] Upserting 1 addresses...\n",
      "2025-10-01 16:39:32,772 [INFO] âœ… Load completed successfully.\n",
      "2025-10-01 16:39:32,824 [INFO] âœ… Updated checkpoint: last_block_height = 80005\n",
      "2025-10-01 16:39:32,825 [INFO] ðŸ“¦ Processing block 80006\n",
      "2025-10-01 16:39:32,826 [INFO] ðŸ”„ Extracting block 80006\n",
      "2025-10-01 16:39:34,533 [INFO] âœ… Got block hash: 00000000000c8650598a1943cc24ed3577dff3b0684e3884a6a47c80270c82af\n",
      "2025-10-01 16:39:38,302 [INFO] ðŸ“¦ Found 1 transactions in block 80006 (first page)\n",
      "2025-10-01 16:39:40,063 [INFO] âœ… Extracted 1 detailed transactions\n",
      "2025-10-01 16:39:40,065 [INFO] ðŸ”„ Transforming data...\n",
      "2025-10-01 16:39:40,086 [INFO] âœ… Transform complete: 1 txs, 1 inputs, 1 outputs, 0 addresses\n",
      "2025-10-01 16:39:40,090 [INFO] ðŸ”„ Loading data into PostgreSQL...\n",
      "2025-10-01 16:39:40,170 [INFO] Inserting 1 transactions...\n",
      "2025-10-01 16:39:40,173 [INFO] Inserting 1 inputs...\n",
      "2025-10-01 16:39:40,177 [INFO] Inserting 1 outputs...\n",
      "2025-10-01 16:39:40,179 [INFO] Upserting 0 addresses...\n",
      "2025-10-01 16:39:40,189 [INFO] âœ… Load completed successfully.\n",
      "2025-10-01 16:39:40,233 [INFO] âœ… Updated checkpoint: last_block_height = 80006\n",
      "2025-10-01 16:39:40,234 [INFO] ðŸ“¦ Processing block 80007\n",
      "2025-10-01 16:39:40,235 [INFO] ðŸ”„ Extracting block 80007\n",
      "2025-10-01 16:39:42,139 [INFO] âœ… Got block hash: 00000000005a786d37b07c4d7ab4c7a93de33a16bb926b4fdd1fb14bf8b8c7a0\n",
      "2025-10-01 16:39:45,657 [INFO] ðŸ“¦ Found 1 transactions in block 80007 (first page)\n",
      "2025-10-01 16:39:47,278 [INFO] âœ… Extracted 1 detailed transactions\n",
      "2025-10-01 16:39:47,278 [INFO] ðŸ”„ Transforming data...\n",
      "2025-10-01 16:39:47,288 [INFO] âœ… Transform complete: 1 txs, 1 inputs, 1 outputs, 0 addresses\n",
      "2025-10-01 16:39:47,303 [INFO] ðŸ”„ Loading data into PostgreSQL...\n",
      "2025-10-01 16:39:47,370 [INFO] Inserting 1 transactions...\n",
      "2025-10-01 16:39:47,370 [INFO] Inserting 1 inputs...\n",
      "2025-10-01 16:39:47,370 [INFO] Inserting 1 outputs...\n",
      "2025-10-01 16:39:47,370 [INFO] Upserting 0 addresses...\n",
      "2025-10-01 16:39:47,392 [INFO] âœ… Load completed successfully.\n",
      "2025-10-01 16:39:47,436 [INFO] âœ… Updated checkpoint: last_block_height = 80007\n",
      "2025-10-01 16:39:47,436 [INFO] ðŸ“¦ Processing block 80008\n",
      "2025-10-01 16:39:47,442 [INFO] ðŸ”„ Extracting block 80008\n",
      "2025-10-01 16:39:49,177 [INFO] âœ… Got block hash: 0000000000547df856decd7a7f23e20f1d98996d78fd1b1801bfd0aa7be022c1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 430\u001b[39m\n\u001b[32m    428\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    429\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m430\u001b[39m         \u001b[43mstream_blocks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpoll_interval\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m30\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    431\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    432\u001b[39m         logger.critical(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mðŸš¨ ETL pipeline crashed: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 416\u001b[39m, in \u001b[36mstream_blocks\u001b[39m\u001b[34m(poll_interval)\u001b[39m\n\u001b[32m    414\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m h \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(last_height + \u001b[32m1\u001b[39m, tip + \u001b[32m1\u001b[39m):\n\u001b[32m    415\u001b[39m     logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mðŸ“¦ Processing block \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mh\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m416\u001b[39m     block_data, detailed_txs = \u001b[43mextract_block\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    417\u001b[39m     block_df, tx_df, inputs_df, outputs_df, address_df = transform_block(block_data, detailed_txs)\n\u001b[32m    418\u001b[39m     load_to_postgres(block_df, tx_df, inputs_df, outputs_df, address_df)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 97\u001b[39m, in \u001b[36mextract_block\u001b[39m\u001b[34m(block_height)\u001b[39m\n\u001b[32m     94\u001b[39m block_hash = requests.get(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mBLOCKSTREAM_API\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/block-height/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mblock_height\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m).text.strip()\n\u001b[32m     95\u001b[39m logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mâœ… Got block hash: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mblock_hash\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m97\u001b[39m block_data = \u001b[43mrequests\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mBLOCKSTREAM_API\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m/block/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mblock_hash\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m.json()\n\u001b[32m     99\u001b[39m txs = requests.get(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mBLOCKSTREAM_API\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/block/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mblock_hash\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/txs\u001b[39m\u001b[33m\"\u001b[39m).json()\n\u001b[32m    100\u001b[39m logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mðŸ“¦ Found \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(txs)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m transactions in block \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mblock_height\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m (first page)\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\USER\\Documents\\PERSONAL DEVELOPMENT\\PROJECTS\\REALTIME_BITCOIN_ETL\\.venv\\Lib\\site-packages\\requests\\api.py:73\u001b[39m, in \u001b[36mget\u001b[39m\u001b[34m(url, params, **kwargs)\u001b[39m\n\u001b[32m     62\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget\u001b[39m(url, params=\u001b[38;5;28;01mNone\u001b[39;00m, **kwargs):\n\u001b[32m     63\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[32m     64\u001b[39m \n\u001b[32m     65\u001b[39m \u001b[33;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     70\u001b[39m \u001b[33;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[32m     71\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mget\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\USER\\Documents\\PERSONAL DEVELOPMENT\\PROJECTS\\REALTIME_BITCOIN_ETL\\.venv\\Lib\\site-packages\\requests\\api.py:59\u001b[39m, in \u001b[36mrequest\u001b[39m\u001b[34m(method, url, **kwargs)\u001b[39m\n\u001b[32m     55\u001b[39m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[32m     56\u001b[39m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[32m     57\u001b[39m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m sessions.Session() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\USER\\Documents\\PERSONAL DEVELOPMENT\\PROJECTS\\REALTIME_BITCOIN_ETL\\.venv\\Lib\\site-packages\\requests\\sessions.py:589\u001b[39m, in \u001b[36mSession.request\u001b[39m\u001b[34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[39m\n\u001b[32m    584\u001b[39m send_kwargs = {\n\u001b[32m    585\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtimeout\u001b[39m\u001b[33m\"\u001b[39m: timeout,\n\u001b[32m    586\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mallow_redirects\u001b[39m\u001b[33m\"\u001b[39m: allow_redirects,\n\u001b[32m    587\u001b[39m }\n\u001b[32m    588\u001b[39m send_kwargs.update(settings)\n\u001b[32m--> \u001b[39m\u001b[32m589\u001b[39m resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    591\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\USER\\Documents\\PERSONAL DEVELOPMENT\\PROJECTS\\REALTIME_BITCOIN_ETL\\.venv\\Lib\\site-packages\\requests\\sessions.py:703\u001b[39m, in \u001b[36mSession.send\u001b[39m\u001b[34m(self, request, **kwargs)\u001b[39m\n\u001b[32m    700\u001b[39m start = preferred_clock()\n\u001b[32m    702\u001b[39m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m703\u001b[39m r = \u001b[43madapter\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    705\u001b[39m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[32m    706\u001b[39m elapsed = preferred_clock() - start\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\USER\\Documents\\PERSONAL DEVELOPMENT\\PROJECTS\\REALTIME_BITCOIN_ETL\\.venv\\Lib\\site-packages\\requests\\adapters.py:644\u001b[39m, in \u001b[36mHTTPAdapter.send\u001b[39m\u001b[34m(self, request, stream, timeout, verify, cert, proxies)\u001b[39m\n\u001b[32m    641\u001b[39m     timeout = TimeoutSauce(connect=timeout, read=timeout)\n\u001b[32m    643\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m644\u001b[39m     resp = \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    645\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    646\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    647\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    648\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    650\u001b[39m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    651\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    652\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    653\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    654\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    655\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    656\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    658\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m    659\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request=request)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\USER\\Documents\\PERSONAL DEVELOPMENT\\PROJECTS\\REALTIME_BITCOIN_ETL\\.venv\\Lib\\site-packages\\urllib3\\connectionpool.py:787\u001b[39m, in \u001b[36mHTTPConnectionPool.urlopen\u001b[39m\u001b[34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[39m\n\u001b[32m    784\u001b[39m response_conn = conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    786\u001b[39m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m787\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    788\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    789\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    790\u001b[39m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    791\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    792\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    793\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    794\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    795\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    796\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    797\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    798\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    799\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    800\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    802\u001b[39m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[32m    803\u001b[39m clean_exit = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\USER\\Documents\\PERSONAL DEVELOPMENT\\PROJECTS\\REALTIME_BITCOIN_ETL\\.venv\\Lib\\site-packages\\urllib3\\connectionpool.py:534\u001b[39m, in \u001b[36mHTTPConnectionPool._make_request\u001b[39m\u001b[34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[39m\n\u001b[32m    532\u001b[39m \u001b[38;5;66;03m# Receive the response from the server\u001b[39;00m\n\u001b[32m    533\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m534\u001b[39m     response = \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    535\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    536\u001b[39m     \u001b[38;5;28mself\u001b[39m._raise_timeout(err=e, url=url, timeout_value=read_timeout)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\USER\\Documents\\PERSONAL DEVELOPMENT\\PROJECTS\\REALTIME_BITCOIN_ETL\\.venv\\Lib\\site-packages\\urllib3\\connection.py:565\u001b[39m, in \u001b[36mHTTPConnection.getresponse\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    562\u001b[39m _shutdown = \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.sock, \u001b[33m\"\u001b[39m\u001b[33mshutdown\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    564\u001b[39m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m565\u001b[39m httplib_response = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    567\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    568\u001b[39m     assert_header_parsing(httplib_response.msg)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\http\\client.py:1428\u001b[39m, in \u001b[36mHTTPConnection.getresponse\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1426\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1427\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1428\u001b[39m         \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1429\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[32m   1430\u001b[39m         \u001b[38;5;28mself\u001b[39m.close()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\http\\client.py:331\u001b[39m, in \u001b[36mHTTPResponse.begin\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    329\u001b[39m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[32m    330\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m331\u001b[39m     version, status, reason = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    332\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m status != CONTINUE:\n\u001b[32m    333\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\http\\client.py:292\u001b[39m, in \u001b[36mHTTPResponse._read_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    291\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m292\u001b[39m     line = \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_MAXLINE\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[33m\"\u001b[39m\u001b[33miso-8859-1\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    293\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) > _MAXLINE:\n\u001b[32m    294\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[33m\"\u001b[39m\u001b[33mstatus line\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\socket.py:720\u001b[39m, in \u001b[36mSocketIO.readinto\u001b[39m\u001b[34m(self, b)\u001b[39m\n\u001b[32m    718\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m    719\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m720\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    721\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[32m    722\u001b[39m         \u001b[38;5;28mself\u001b[39m._timeout_occurred = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\ssl.py:1251\u001b[39m, in \u001b[36mSSLSocket.recv_into\u001b[39m\u001b[34m(self, buffer, nbytes, flags)\u001b[39m\n\u001b[32m   1247\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m flags != \u001b[32m0\u001b[39m:\n\u001b[32m   1248\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1249\u001b[39m           \u001b[33m\"\u001b[39m\u001b[33mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m %\n\u001b[32m   1250\u001b[39m           \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1251\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1252\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1253\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().recv_into(buffer, nbytes, flags)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\ssl.py:1103\u001b[39m, in \u001b[36mSSLSocket.read\u001b[39m\u001b[34m(self, len, buffer)\u001b[39m\n\u001b[32m   1101\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1102\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1103\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sslobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1104\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1105\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sslobj.read(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import psycopg2\n",
    "import pandas as pd\n",
    "import logging\n",
    "import sys\n",
    "import traceback\n",
    "import time\n",
    "\n",
    "# ------------------------------\n",
    "# CONFIGURATION\n",
    "# ------------------------------\n",
    "BLOCKSTREAM_API = \"https://blockstream.info/api\"\n",
    "DB_HOST = \"localhost\"\n",
    "DB_PORT = \"5432\"\n",
    "DB_NAME = \"bitcoin_db\"\n",
    "DB_USER = \"postgres\"\n",
    "DB_PASS = \"chibuoke3456\"\n",
    "\n",
    "# ------------------------------\n",
    "# LOGGING SETUP\n",
    "# ------------------------------\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s [%(levelname)s] %(message)s\",\n",
    "    handlers=[\n",
    "        logging.StreamHandler(sys.stdout),\n",
    "        logging.FileHandler(\"etl_bitcoin.log\", mode=\"a\")\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "# ------------------------------\n",
    "# DB HELPERS\n",
    "# ------------------------------\n",
    "def get_connection():\n",
    "    return psycopg2.connect(\n",
    "        host=DB_HOST, port=DB_PORT,\n",
    "        dbname=DB_NAME, user=DB_USER, password=DB_PASS\n",
    "    )\n",
    "\n",
    "\n",
    "def ensure_state_table():\n",
    "    \"\"\"Create table to track last processed block height\"\"\"\n",
    "    conn = get_connection()\n",
    "    cur = conn.cursor()\n",
    "    cur.execute(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS etl_state (\n",
    "        id SERIAL PRIMARY KEY,\n",
    "        last_block_height BIGINT\n",
    "    );\n",
    "    \"\"\")\n",
    "    conn.commit()\n",
    "    cur.close()\n",
    "    conn.close()\n",
    "\n",
    "\n",
    "def get_last_processed_height():\n",
    "    \"\"\"Fetch last processed block height from Postgres\"\"\"\n",
    "    conn = get_connection()\n",
    "    cur = conn.cursor()\n",
    "    cur.execute(\"SELECT last_block_height FROM etl_state ORDER BY id DESC LIMIT 1;\")\n",
    "    row = cur.fetchone()\n",
    "    cur.close()\n",
    "    conn.close()\n",
    "    return row[0] if row else None\n",
    "\n",
    "\n",
    "def update_last_processed_height(height: int):\n",
    "    \"\"\"Update last processed block height\"\"\"\n",
    "    conn = get_connection()\n",
    "    cur = conn.cursor()\n",
    "    cur.execute(\"INSERT INTO etl_state (last_block_height) VALUES (%s);\", (height,))\n",
    "    conn.commit()\n",
    "    cur.close()\n",
    "    conn.close()\n",
    "    logger.info(f\"âœ… Updated checkpoint: last_block_height = {height}\")\n",
    "\n",
    "\n",
    "# ------------------------------\n",
    "# GET LATEST BLOCK HEIGHT\n",
    "# ------------------------------\n",
    "def get_latest_height():\n",
    "    return int(requests.get(f\"{BLOCKSTREAM_API}/blocks/tip/height\").text.strip())\n",
    "\n",
    "\n",
    "# ------------------------------\n",
    "# EXTRACT\n",
    "# ------------------------------\n",
    "def extract_block(block_height: int):\n",
    "    try:\n",
    "        logger.info(f\"ðŸ”„ Extracting block {block_height}\")\n",
    "\n",
    "        block_hash = requests.get(f\"{BLOCKSTREAM_API}/block-height/{block_height}\").text.strip()\n",
    "        logger.info(f\"âœ… Got block hash: {block_hash}\")\n",
    "\n",
    "        block_data = requests.get(f\"{BLOCKSTREAM_API}/block/{block_hash}\").json()\n",
    "\n",
    "        txs = requests.get(f\"{BLOCKSTREAM_API}/block/{block_hash}/txs\").json()\n",
    "        logger.info(f\"ðŸ“¦ Found {len(txs)} transactions in block {block_height} (first page)\")\n",
    "\n",
    "        detailed_txs = []\n",
    "        for tx in txs:\n",
    "            txid = tx[\"txid\"]\n",
    "            try:\n",
    "                detailed_tx = requests.get(f\"{BLOCKSTREAM_API}/tx/{txid}\").json()\n",
    "                detailed_txs.append(detailed_tx)\n",
    "            except Exception as e:\n",
    "                logger.error(f\"âš ï¸ Failed to fetch tx {txid}: {e}\")\n",
    "                logger.error(traceback.format_exc())\n",
    "\n",
    "        logger.info(f\"âœ… Extracted {len(detailed_txs)} detailed transactions\")\n",
    "        return block_data, detailed_txs\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"âŒ Extract step failed: {e}\")\n",
    "        logger.error(traceback.format_exc())\n",
    "        raise\n",
    "\n",
    "\n",
    "# ------------------------------\n",
    "# TRANSFORM\n",
    "# ------------------------------\n",
    "def transform_block(block_data, detailed_txs):\n",
    "    try:\n",
    "        logger.info(\"ðŸ”„ Transforming data...\")\n",
    "\n",
    "        block_df = pd.DataFrame([{\n",
    "            \"block_id\": block_data[\"id\"],\n",
    "            \"height\": block_data[\"height\"],\n",
    "            \"timestamp\": pd.to_datetime(block_data[\"timestamp\"], unit=\"s\"),\n",
    "            \"tx_count\": block_data[\"tx_count\"],\n",
    "            \"size\": block_data[\"size\"],\n",
    "            \"weight\": block_data[\"weight\"]\n",
    "        }])\n",
    "\n",
    "        tx_list, input_list, output_list = [], [], []\n",
    "        address_dict = {}\n",
    "\n",
    "        for tx in detailed_txs:\n",
    "            tx_time = pd.to_datetime(tx.get(\"status\", {}).get(\"block_time\"), unit=\"s\")\n",
    "\n",
    "            tx_list.append({\n",
    "                \"txid\": tx[\"txid\"],\n",
    "                \"block_id\": block_data[\"id\"],\n",
    "                \"fee\": tx.get(\"fee\"),\n",
    "                \"size\": tx.get(\"size\"),\n",
    "                \"weight\": tx.get(\"weight\"),\n",
    "                \"version\": tx.get(\"version\"),\n",
    "                \"locktime\": tx.get(\"locktime\")\n",
    "            })\n",
    "\n",
    "            # Inputs\n",
    "            for vin in tx.get(\"vin\", []):\n",
    "                prevout = vin.get(\"prevout\") or {}\n",
    "                input_list.append({\n",
    "                    \"txid\": tx[\"txid\"],\n",
    "                    \"prev_txid\": vin.get(\"txid\"),\n",
    "                    \"prev_index\": vin.get(\"vout\"),\n",
    "                    \"input_address\": prevout.get(\"scriptpubkey_address\"),\n",
    "                    \"input_value\": prevout.get(\"value\")\n",
    "                })\n",
    "\n",
    "                addr = prevout.get(\"scriptpubkey_address\")\n",
    "                if addr:\n",
    "                    address_dict.setdefault(addr, {\n",
    "                        \"total_received\": 0, \"total_sent\": 0,\n",
    "                        \"first_seen\": tx_time, \"last_seen\": tx_time\n",
    "                    })\n",
    "                    address_dict[addr][\"total_sent\"] += prevout.get(\"value\", 0)\n",
    "                    address_dict[addr][\"last_seen\"] = tx_time\n",
    "\n",
    "            # Outputs\n",
    "            for idx, vout in enumerate(tx.get(\"vout\", [])):\n",
    "                output_list.append({\n",
    "                    \"txid\": tx[\"txid\"],\n",
    "                    \"vout_index\": idx,\n",
    "                    \"output_address\": vout.get(\"scriptpubkey_address\"),\n",
    "                    \"output_value\": vout.get(\"value\"),\n",
    "                    \"spent\": vout.get(\"spent\", False)\n",
    "                })\n",
    "\n",
    "                addr = vout.get(\"scriptpubkey_address\")\n",
    "                if addr:\n",
    "                    address_dict.setdefault(addr, {\n",
    "                        \"total_received\": 0, \"total_sent\": 0,\n",
    "                        \"first_seen\": tx_time, \"last_seen\": tx_time\n",
    "                    })\n",
    "                    address_dict[addr][\"total_received\"] += vout.get(\"value\", 0)\n",
    "                    if address_dict[addr][\"first_seen\"] is None:\n",
    "                        address_dict[addr][\"first_seen\"] = tx_time\n",
    "                    address_dict[addr][\"last_seen\"] = tx_time\n",
    "\n",
    "        tx_df = pd.DataFrame(tx_list)\n",
    "        inputs_df = pd.DataFrame(input_list)\n",
    "        outputs_df = pd.DataFrame(output_list)\n",
    "\n",
    "        addr_list = []\n",
    "        for addr, stats in address_dict.items():\n",
    "            balance = stats[\"total_received\"] - stats[\"total_sent\"]\n",
    "            addr_list.append({\n",
    "                \"address\": addr,\n",
    "                \"total_received\": stats[\"total_received\"],\n",
    "                \"total_sent\": stats[\"total_sent\"],\n",
    "                \"balance\": balance,\n",
    "                \"first_seen\": stats[\"first_seen\"],\n",
    "                \"last_seen\": stats[\"last_seen\"]\n",
    "            })\n",
    "        address_df = pd.DataFrame(addr_list)\n",
    "\n",
    "        logger.info(f\"âœ… Transform complete: {len(tx_df)} txs, {len(inputs_df)} inputs, {len(outputs_df)} outputs, {len(address_df)} addresses\")\n",
    "        return block_df, tx_df, inputs_df, outputs_df, address_df\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"âŒ Transform step failed: {e}\")\n",
    "        logger.error(traceback.format_exc())\n",
    "        raise\n",
    "\n",
    "\n",
    "# ------------------------------\n",
    "# LOAD (same as before)\n",
    "# ------------------------------\n",
    "def load_to_postgres(block_df, tx_df, inputs_df, outputs_df, address_df):\n",
    "    try:\n",
    "        logger.info(\"ðŸ”„ Loading data into PostgreSQL...\")\n",
    "\n",
    "        conn = psycopg2.connect(\n",
    "            host=DB_HOST, port=DB_PORT,\n",
    "            dbname=DB_NAME, user=DB_USER, password=DB_PASS\n",
    "        )\n",
    "        cursor = conn.cursor()\n",
    "\n",
    "        # Create tables\n",
    "        cursor.execute(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS bitcoin_blocks (\n",
    "            block_id TEXT PRIMARY KEY,\n",
    "            height NUMERIC,\n",
    "            timestamp TIMESTAMP,\n",
    "            tx_count NUMERIC,\n",
    "            size BIGINT,\n",
    "            weight BIGINT\n",
    "        );\n",
    "        \"\"\")\n",
    "        cursor.execute(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS bitcoin_transactions (\n",
    "            txid TEXT PRIMARY KEY,\n",
    "            block_id TEXT,\n",
    "            fee NUMERIC,\n",
    "            size BIGINT,\n",
    "            weight BIGINT,\n",
    "            version NUMERIC,\n",
    "            locktime BIGINT,\n",
    "            FOREIGN KEY(block_id) REFERENCES bitcoin_blocks(block_id)\n",
    "        );\n",
    "        \"\"\")\n",
    "        cursor.execute(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS bitcoin_inputs (\n",
    "            id SERIAL PRIMARY KEY,\n",
    "            txid TEXT,\n",
    "            prev_txid TEXT,\n",
    "            prev_index NUMERIC,\n",
    "            input_address TEXT,\n",
    "            input_value NUMERIC\n",
    "        );\n",
    "        \"\"\")\n",
    "        cursor.execute(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS bitcoin_outputs (\n",
    "            id SERIAL PRIMARY KEY,\n",
    "            txid TEXT,\n",
    "            vout_index NUMERIC,\n",
    "            output_address TEXT,\n",
    "            output_value NUMERIC,\n",
    "            spent BOOLEAN\n",
    "        );\n",
    "        \"\"\")\n",
    "        cursor.execute(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS bitcoin_addresses (\n",
    "            address TEXT PRIMARY KEY,\n",
    "            total_received NUMERIC,\n",
    "            total_sent NUMERIC,\n",
    "            balance NUMERIC,\n",
    "            first_seen TIMESTAMP,\n",
    "            last_seen TIMESTAMP\n",
    "        );\n",
    "        \"\"\")\n",
    "\n",
    "        # Insert block\n",
    "        for _, row in block_df.iterrows():\n",
    "            try:\n",
    "                cursor.execute(\"\"\"\n",
    "                INSERT INTO bitcoin_blocks (block_id, height, timestamp, tx_count, size, weight)\n",
    "                VALUES (%s, %s, %s, %s, %s, %s)\n",
    "                ON CONFLICT (block_id) DO NOTHING;\n",
    "                \"\"\", (row.block_id, row.height, row.timestamp, row.tx_count, row.size, row.weight))\n",
    "            except Exception as e:\n",
    "                logger.error(f\"âŒ Failed inserting block: {e}\")\n",
    "                logger.error(traceback.format_exc())\n",
    "\n",
    "        # Insert transactions\n",
    "        logger.info(f\"Inserting {len(tx_df)} transactions...\")\n",
    "        for _, row in tx_df.iterrows():\n",
    "            try:\n",
    "                cursor.execute(\"\"\"\n",
    "                INSERT INTO bitcoin_transactions (txid, block_id, fee, size, weight, version, locktime)\n",
    "                VALUES (%s, %s, %s, %s, %s, %s, %s)\n",
    "                ON CONFLICT (txid) DO NOTHING;\n",
    "                \"\"\", (row.txid, row.block_id, row.fee, row.size, row.weight, row.version, row.locktime))\n",
    "            except Exception as e:\n",
    "                logger.error(f\"âŒ Failed inserting transaction {row.txid}: {e}\")\n",
    "                logger.error(traceback.format_exc())\n",
    "\n",
    "        # Insert inputs\n",
    "        logger.info(f\"Inserting {len(inputs_df)} inputs...\")\n",
    "        for _, row in inputs_df.iterrows():\n",
    "            try:\n",
    "                cursor.execute(\"\"\"\n",
    "                INSERT INTO bitcoin_inputs (txid, prev_txid, prev_index, input_address, input_value)\n",
    "                VALUES (%s, %s, %s, %s, %s)\n",
    "                \"\"\", (row.txid, row.prev_txid, row.prev_index, row.input_address, row.input_value))\n",
    "            except Exception as e:\n",
    "                logger.error(f\"âŒ Failed inserting input for tx {row.txid}: {e}\")\n",
    "                logger.error(traceback.format_exc())\n",
    "\n",
    "        # Insert outputs\n",
    "        logger.info(f\"Inserting {len(outputs_df)} outputs...\")\n",
    "        for _, row in outputs_df.iterrows():\n",
    "            try:\n",
    "                cursor.execute(\"\"\"\n",
    "                INSERT INTO bitcoin_outputs (txid, vout_index, output_address, output_value, spent)\n",
    "                VALUES (%s, %s, %s, %s, %s)\n",
    "                \"\"\", (row.txid, row.vout_index, row.output_address, row.output_value, row.spent))\n",
    "            except Exception as e:\n",
    "                logger.error(f\"âŒ Failed inserting output for tx {row.txid}: {e}\")\n",
    "                logger.error(traceback.format_exc())\n",
    "\n",
    "        # Insert/update addresses\n",
    "        logger.info(f\"Upserting {len(address_df)} addresses...\")\n",
    "        for _, row in address_df.iterrows():\n",
    "            try:\n",
    "                cursor.execute(\"\"\"\n",
    "                INSERT INTO bitcoin_addresses (address, total_received, total_sent, balance, first_seen, last_seen)\n",
    "                VALUES (%s, %s, %s, %s, %s, %s)\n",
    "                ON CONFLICT (address) \n",
    "                DO UPDATE SET total_received = bitcoin_addresses.total_received + EXCLUDED.total_received,\n",
    "                              total_sent = bitcoin_addresses.total_sent + EXCLUDED.total_sent,\n",
    "                              balance = bitcoin_addresses.balance + EXCLUDED.balance,\n",
    "                              last_seen = GREATEST(bitcoin_addresses.last_seen, EXCLUDED.last_seen);\n",
    "                \"\"\", (row.address, row.total_received, row.total_sent, row.balance, row.first_seen, row.last_seen))\n",
    "            except Exception as e:\n",
    "                logger.error(f\"âŒ Failed upserting address {row.address}: {e}\")\n",
    "                logger.error(traceback.format_exc())\n",
    "\n",
    "        conn.commit()\n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "        logger.info(\"âœ… Load completed successfully.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"âŒ Load step failed: {e}\")\n",
    "        logger.error(traceback.format_exc())\n",
    "        raise\n",
    "\n",
    "\n",
    "# ------------------------------\n",
    "# STREAMING WITH CHECKPOINT\n",
    "# ------------------------------\n",
    "# def stream_blocks(poll_interval=30):\n",
    "#     \"\"\"Continuously fetch new blocks from last checkpoint\"\"\"\n",
    "#     ensure_state_table()\n",
    "#     last_height = get_last_processed_height()\n",
    "\n",
    "#     if last_height:\n",
    "#         logger.info(f\"ðŸ“Œ Resuming from checkpoint block {last_height}\")\n",
    "#     else:\n",
    "#         last_height = get_latest_height()\n",
    "#         logger.info(f\"ðŸš€ Starting fresh from tip block {last_height}\")\n",
    "#         update_last_processed_height(last_height)\n",
    "\n",
    "#     while True:\n",
    "#         tip = get_latest_height()\n",
    "#         if last_height < tip:\n",
    "#             for h in range(last_height + 1, tip + 1):\n",
    "#                 logger.info(f\"ðŸ“¦ Processing block {h}\")\n",
    "#                 block_data, detailed_txs = extract_block(h)\n",
    "#                 block_df, tx_df, inputs_df, outputs_df, address_df = transform_block(block_data, detailed_txs)\n",
    "#                 load_to_postgres(block_df, tx_df, inputs_df, outputs_df, address_df)\n",
    "#                 update_last_processed_height(h)\n",
    "#                 last_height = h\n",
    "#         else:\n",
    "#             logger.info(\"â³ No new block yet...\")\n",
    "#         time.sleep(poll_interval)\n",
    "\n",
    "# ------------------------------\n",
    "# STREAMING WITH CHECKPOINT\n",
    "# ------------------------------\n",
    "START_HEIGHT = 80000  # <--- your chosen starting block\n",
    "\n",
    "def stream_blocks(poll_interval=30):\n",
    "    \"\"\"Continuously fetch new blocks from last checkpoint\"\"\"\n",
    "    ensure_state_table()\n",
    "    last_height = get_last_processed_height()\n",
    "\n",
    "    if last_height:\n",
    "        logger.info(f\"ðŸ“Œ Resuming from checkpoint block {last_height}\")\n",
    "    else:\n",
    "        # If no checkpoint exists, start from your chosen height\n",
    "        last_height = START_HEIGHT\n",
    "        logger.info(f\"ðŸš€ Starting fresh from configured block {START_HEIGHT}\")\n",
    "        update_last_processed_height(last_height)\n",
    "\n",
    "    while True:\n",
    "        tip = get_latest_height()\n",
    "        if last_height < tip:\n",
    "            for h in range(last_height + 1, tip + 1):\n",
    "                logger.info(f\"ðŸ“¦ Processing block {h}\")\n",
    "                block_data, detailed_txs = extract_block(h)\n",
    "                block_df, tx_df, inputs_df, outputs_df, address_df = transform_block(block_data, detailed_txs)\n",
    "                load_to_postgres(block_df, tx_df, inputs_df, outputs_df, address_df)\n",
    "                update_last_processed_height(h)\n",
    "                last_height = h\n",
    "        else:\n",
    "            logger.info(\"â³ No new block yet...\")\n",
    "        time.sleep(poll_interval)\n",
    "\n",
    "# ------------------------------\n",
    "# MAIN\n",
    "# ------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        stream_blocks(poll_interval=30)\n",
    "    except Exception as e:\n",
    "        logger.critical(f\"ðŸš¨ ETL pipeline crashed: {e}\")\n",
    "        logger.critical(traceback.format_exc())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b15207e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39abd688",
   "metadata": {},
   "outputs": [],
   "source": [
    "# streaming data based on start date or block height for better flexibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b6c469f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-01 16:56:47,265 [INFO] ðŸ“… Using block 917241 from 2021-01-01\n",
      "2025-10-01 16:56:47,265 [INFO] ðŸš€ Starting from first block of date 2021-01-01 (height 917241)\n",
      "2025-10-01 16:56:47,532 [INFO] âœ… Updated checkpoint: last_block_height = 917241\n",
      "2025-10-01 16:56:49,284 [INFO] ðŸ“¦ Processing block 917242\n",
      "2025-10-01 16:56:49,284 [INFO] ðŸ”„ Extracting block 917242\n",
      "2025-10-01 16:56:50,924 [INFO] âœ… Got block hash: 0000000000000000000074b28292653f859653b856026e15145768b287c1a481\n",
      "2025-10-01 16:56:54,916 [INFO] ðŸ“¦ Found 25 transactions in block 917242 (first page)\n",
      "2025-10-01 16:57:42,217 [INFO] âœ… Extracted 25 detailed transactions\n",
      "2025-10-01 16:57:42,235 [INFO] ðŸ”„ Transforming data...\n",
      "2025-10-01 16:57:42,265 [INFO] âœ… Transform complete: 25 txs, 29 inputs, 111 outputs, 111 addresses\n",
      "2025-10-01 16:57:42,269 [INFO] ðŸ”„ Loading data into PostgreSQL...\n",
      "2025-10-01 16:57:42,363 [INFO] Inserting 25 transactions...\n",
      "2025-10-01 16:57:42,378 [INFO] Inserting 29 inputs...\n",
      "2025-10-01 16:57:42,402 [INFO] Inserting 111 outputs...\n",
      "2025-10-01 16:57:42,497 [INFO] Upserting 111 addresses...\n",
      "2025-10-01 16:57:42,665 [INFO] âœ… Load completed successfully.\n",
      "2025-10-01 16:57:42,763 [INFO] âœ… Updated checkpoint: last_block_height = 917242\n",
      "2025-10-01 16:57:42,763 [INFO] ðŸ“¦ Processing block 917243\n",
      "2025-10-01 16:57:42,775 [INFO] ðŸ”„ Extracting block 917243\n",
      "2025-10-01 16:57:44,992 [INFO] âœ… Got block hash: 000000000000000000002e7e467cc20287a7fc31a00a3e8f1d8685cd8dc25850\n",
      "2025-10-01 16:57:48,980 [INFO] ðŸ“¦ Found 25 transactions in block 917243 (first page)\n",
      "2025-10-01 16:58:39,717 [INFO] âœ… Extracted 25 detailed transactions\n",
      "2025-10-01 16:58:39,717 [INFO] ðŸ”„ Transforming data...\n",
      "2025-10-01 16:58:39,724 [INFO] âœ… Transform complete: 25 txs, 37 inputs, 46 outputs, 68 addresses\n",
      "2025-10-01 16:58:39,724 [INFO] ðŸ”„ Loading data into PostgreSQL...\n",
      "2025-10-01 16:58:39,790 [INFO] Inserting 25 transactions...\n",
      "2025-10-01 16:58:39,809 [INFO] Inserting 37 inputs...\n",
      "2025-10-01 16:58:39,833 [INFO] Inserting 46 outputs...\n",
      "2025-10-01 16:58:39,863 [INFO] Upserting 68 addresses...\n",
      "2025-10-01 16:58:39,921 [INFO] âœ… Load completed successfully.\n",
      "2025-10-01 16:58:40,006 [INFO] âœ… Updated checkpoint: last_block_height = 917243\n",
      "2025-10-01 16:58:40,006 [INFO] ðŸ“¦ Processing block 917244\n",
      "2025-10-01 16:58:40,009 [INFO] ðŸ”„ Extracting block 917244\n",
      "2025-10-01 16:58:41,918 [INFO] âœ… Got block hash: 00000000000000000001d7d619948ecd4eac83a0a47382fb11b19cc069e8faf0\n",
      "2025-10-01 16:58:46,152 [INFO] ðŸ“¦ Found 25 transactions in block 917244 (first page)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 463\u001b[39m\n\u001b[32m    461\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    462\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m463\u001b[39m         \u001b[43mstream_blocks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpoll_interval\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m30\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    464\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    465\u001b[39m         logger.critical(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mðŸš¨ ETL pipeline crashed: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 449\u001b[39m, in \u001b[36mstream_blocks\u001b[39m\u001b[34m(poll_interval)\u001b[39m\n\u001b[32m    447\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m h \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(last_height + \u001b[32m1\u001b[39m, tip + \u001b[32m1\u001b[39m):\n\u001b[32m    448\u001b[39m     logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mðŸ“¦ Processing block \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mh\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m449\u001b[39m     block_data, detailed_txs = \u001b[43mextract_block\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    450\u001b[39m     block_df, tx_df, inputs_df, outputs_df, address_df = transform_block(block_data, detailed_txs)\n\u001b[32m    451\u001b[39m     load_to_postgres(block_df, tx_df, inputs_df, outputs_df, address_df)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 132\u001b[39m, in \u001b[36mextract_block\u001b[39m\u001b[34m(block_height)\u001b[39m\n\u001b[32m    130\u001b[39m txid = tx[\u001b[33m\"\u001b[39m\u001b[33mtxid\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    131\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m132\u001b[39m     detailed_tx = \u001b[43mrequests\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mBLOCKSTREAM_API\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m/tx/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mtxid\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m.json()\n\u001b[32m    133\u001b[39m     detailed_txs.append(detailed_tx)\n\u001b[32m    134\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\USER\\Documents\\PERSONAL DEVELOPMENT\\PROJECTS\\REALTIME_BITCOIN_ETL\\.venv\\Lib\\site-packages\\requests\\api.py:73\u001b[39m, in \u001b[36mget\u001b[39m\u001b[34m(url, params, **kwargs)\u001b[39m\n\u001b[32m     62\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget\u001b[39m(url, params=\u001b[38;5;28;01mNone\u001b[39;00m, **kwargs):\n\u001b[32m     63\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[32m     64\u001b[39m \n\u001b[32m     65\u001b[39m \u001b[33;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     70\u001b[39m \u001b[33;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[32m     71\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mget\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\USER\\Documents\\PERSONAL DEVELOPMENT\\PROJECTS\\REALTIME_BITCOIN_ETL\\.venv\\Lib\\site-packages\\requests\\api.py:59\u001b[39m, in \u001b[36mrequest\u001b[39m\u001b[34m(method, url, **kwargs)\u001b[39m\n\u001b[32m     55\u001b[39m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[32m     56\u001b[39m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[32m     57\u001b[39m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m sessions.Session() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\USER\\Documents\\PERSONAL DEVELOPMENT\\PROJECTS\\REALTIME_BITCOIN_ETL\\.venv\\Lib\\site-packages\\requests\\sessions.py:589\u001b[39m, in \u001b[36mSession.request\u001b[39m\u001b[34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[39m\n\u001b[32m    584\u001b[39m send_kwargs = {\n\u001b[32m    585\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtimeout\u001b[39m\u001b[33m\"\u001b[39m: timeout,\n\u001b[32m    586\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mallow_redirects\u001b[39m\u001b[33m\"\u001b[39m: allow_redirects,\n\u001b[32m    587\u001b[39m }\n\u001b[32m    588\u001b[39m send_kwargs.update(settings)\n\u001b[32m--> \u001b[39m\u001b[32m589\u001b[39m resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    591\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\USER\\Documents\\PERSONAL DEVELOPMENT\\PROJECTS\\REALTIME_BITCOIN_ETL\\.venv\\Lib\\site-packages\\requests\\sessions.py:703\u001b[39m, in \u001b[36mSession.send\u001b[39m\u001b[34m(self, request, **kwargs)\u001b[39m\n\u001b[32m    700\u001b[39m start = preferred_clock()\n\u001b[32m    702\u001b[39m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m703\u001b[39m r = \u001b[43madapter\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    705\u001b[39m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[32m    706\u001b[39m elapsed = preferred_clock() - start\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\USER\\Documents\\PERSONAL DEVELOPMENT\\PROJECTS\\REALTIME_BITCOIN_ETL\\.venv\\Lib\\site-packages\\requests\\adapters.py:644\u001b[39m, in \u001b[36mHTTPAdapter.send\u001b[39m\u001b[34m(self, request, stream, timeout, verify, cert, proxies)\u001b[39m\n\u001b[32m    641\u001b[39m     timeout = TimeoutSauce(connect=timeout, read=timeout)\n\u001b[32m    643\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m644\u001b[39m     resp = \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    645\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    646\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    647\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    648\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    650\u001b[39m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    651\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    652\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    653\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    654\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    655\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    656\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    658\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m    659\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request=request)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\USER\\Documents\\PERSONAL DEVELOPMENT\\PROJECTS\\REALTIME_BITCOIN_ETL\\.venv\\Lib\\site-packages\\urllib3\\connectionpool.py:787\u001b[39m, in \u001b[36mHTTPConnectionPool.urlopen\u001b[39m\u001b[34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[39m\n\u001b[32m    784\u001b[39m response_conn = conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    786\u001b[39m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m787\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    788\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    789\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    790\u001b[39m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    791\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    792\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    793\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    794\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    795\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    796\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    797\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    798\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    799\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    800\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    802\u001b[39m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[32m    803\u001b[39m clean_exit = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\USER\\Documents\\PERSONAL DEVELOPMENT\\PROJECTS\\REALTIME_BITCOIN_ETL\\.venv\\Lib\\site-packages\\urllib3\\connectionpool.py:534\u001b[39m, in \u001b[36mHTTPConnectionPool._make_request\u001b[39m\u001b[34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[39m\n\u001b[32m    532\u001b[39m \u001b[38;5;66;03m# Receive the response from the server\u001b[39;00m\n\u001b[32m    533\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m534\u001b[39m     response = \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    535\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    536\u001b[39m     \u001b[38;5;28mself\u001b[39m._raise_timeout(err=e, url=url, timeout_value=read_timeout)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\USER\\Documents\\PERSONAL DEVELOPMENT\\PROJECTS\\REALTIME_BITCOIN_ETL\\.venv\\Lib\\site-packages\\urllib3\\connection.py:565\u001b[39m, in \u001b[36mHTTPConnection.getresponse\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    562\u001b[39m _shutdown = \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.sock, \u001b[33m\"\u001b[39m\u001b[33mshutdown\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    564\u001b[39m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m565\u001b[39m httplib_response = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    567\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    568\u001b[39m     assert_header_parsing(httplib_response.msg)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\http\\client.py:1428\u001b[39m, in \u001b[36mHTTPConnection.getresponse\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1426\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1427\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1428\u001b[39m         \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1429\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[32m   1430\u001b[39m         \u001b[38;5;28mself\u001b[39m.close()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\http\\client.py:331\u001b[39m, in \u001b[36mHTTPResponse.begin\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    329\u001b[39m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[32m    330\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m331\u001b[39m     version, status, reason = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    332\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m status != CONTINUE:\n\u001b[32m    333\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\http\\client.py:292\u001b[39m, in \u001b[36mHTTPResponse._read_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    291\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m292\u001b[39m     line = \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_MAXLINE\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[33m\"\u001b[39m\u001b[33miso-8859-1\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    293\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) > _MAXLINE:\n\u001b[32m    294\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[33m\"\u001b[39m\u001b[33mstatus line\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\socket.py:720\u001b[39m, in \u001b[36mSocketIO.readinto\u001b[39m\u001b[34m(self, b)\u001b[39m\n\u001b[32m    718\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m    719\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m720\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    721\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[32m    722\u001b[39m         \u001b[38;5;28mself\u001b[39m._timeout_occurred = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\ssl.py:1251\u001b[39m, in \u001b[36mSSLSocket.recv_into\u001b[39m\u001b[34m(self, buffer, nbytes, flags)\u001b[39m\n\u001b[32m   1247\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m flags != \u001b[32m0\u001b[39m:\n\u001b[32m   1248\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1249\u001b[39m           \u001b[33m\"\u001b[39m\u001b[33mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m %\n\u001b[32m   1250\u001b[39m           \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1251\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1252\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1253\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().recv_into(buffer, nbytes, flags)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\ssl.py:1103\u001b[39m, in \u001b[36mSSLSocket.read\u001b[39m\u001b[34m(self, len, buffer)\u001b[39m\n\u001b[32m   1101\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1102\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1103\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sslobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1104\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1105\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sslobj.read(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import psycopg2\n",
    "import pandas as pd\n",
    "import logging\n",
    "import sys\n",
    "import traceback\n",
    "import time\n",
    "\n",
    "# ------------------------------\n",
    "# CONFIGURATION\n",
    "# ------------------------------\n",
    "BLOCKSTREAM_API = \"https://blockstream.info/api\"\n",
    "DB_HOST = \"localhost\"\n",
    "DB_PORT = \"5432\"\n",
    "DB_NAME = \"bitcoin_db\"\n",
    "DB_USER = \"postgres\"\n",
    "DB_PASS = \"chibuoke3456\"\n",
    "\n",
    "START_HEIGHT = None   # set to an int like 80000 if you want height-based start\n",
    "START_DATE = \"2021-01-01\"  # set to a YYYY-MM-DD string if you want date-based start\n",
    "\n",
    "# ------------------------------\n",
    "# LOGGING SETUP\n",
    "# ------------------------------\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s [%(levelname)s] %(message)s\",\n",
    "    handlers=[\n",
    "        logging.StreamHandler(sys.stdout),\n",
    "        logging.FileHandler(\"etl_bitcoin.log\", mode=\"a\")\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "# ------------------------------\n",
    "# DB HELPERS\n",
    "# ------------------------------\n",
    "def get_connection():\n",
    "    return psycopg2.connect(\n",
    "        host=DB_HOST, port=DB_PORT,\n",
    "        dbname=DB_NAME, user=DB_USER, password=DB_PASS\n",
    "    )\n",
    "\n",
    "\n",
    "def ensure_state_table():\n",
    "    \"\"\"Create table to track last processed block height\"\"\"\n",
    "    conn = get_connection()\n",
    "    cur = conn.cursor()\n",
    "    cur.execute(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS etl_state (\n",
    "        id SERIAL PRIMARY KEY,\n",
    "        last_block_height BIGINT\n",
    "    );\n",
    "    \"\"\")\n",
    "    conn.commit()\n",
    "    cur.close()\n",
    "    conn.close()\n",
    "\n",
    "\n",
    "def get_last_processed_height():\n",
    "    \"\"\"Fetch last processed block height from Postgres\"\"\"\n",
    "    conn = get_connection()\n",
    "    cur = conn.cursor()\n",
    "    cur.execute(\"SELECT last_block_height FROM etl_state ORDER BY id DESC LIMIT 1;\")\n",
    "    row = cur.fetchone()\n",
    "    cur.close()\n",
    "    conn.close()\n",
    "    return row[0] if row else None\n",
    "\n",
    "\n",
    "def update_last_processed_height(height: int):\n",
    "    \"\"\"Update last processed block height\"\"\"\n",
    "    conn = get_connection()\n",
    "    cur = conn.cursor()\n",
    "    cur.execute(\"INSERT INTO etl_state (last_block_height) VALUES (%s);\", (height,))\n",
    "    conn.commit()\n",
    "    cur.close()\n",
    "    conn.close()\n",
    "    logger.info(f\"âœ… Updated checkpoint: last_block_height = {height}\")\n",
    "\n",
    "\n",
    "\n",
    "#------------------------------\n",
    "#HELPER FUNCTION TO FIND START HEIGHT BASED ON DATE\n",
    "#------------------------------\n",
    "\n",
    "def get_height_from_date(date_str: str):\n",
    "    \"\"\"\n",
    "    Get the first block height from a given date (YYYY-MM-DD).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        url = f\"{BLOCKSTREAM_API}/blocks/{date_str}\"\n",
    "        resp = requests.get(url).json()\n",
    "        if not resp:\n",
    "            raise Exception(f\"No blocks found for {date_str}\")\n",
    "        # take the earliest block that day\n",
    "        block = resp[-1]  # API returns newest first, so last one is earliest\n",
    "        logger.info(f\"ðŸ“… Using block {block['height']} from {date_str}\")\n",
    "        return block[\"height\"]\n",
    "    except Exception as e:\n",
    "        logger.error(f\"âŒ Failed to fetch block height for date {date_str}: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "# ------------------------------\n",
    "# GET LATEST BLOCK HEIGHT\n",
    "# ------------------------------\n",
    "def get_latest_height():\n",
    "    return int(requests.get(f\"{BLOCKSTREAM_API}/blocks/tip/height\").text.strip())\n",
    "\n",
    "\n",
    "# ------------------------------\n",
    "# EXTRACT\n",
    "# ------------------------------\n",
    "def extract_block(block_height: int):\n",
    "    try:\n",
    "        logger.info(f\"ðŸ”„ Extracting block {block_height}\")\n",
    "\n",
    "        block_hash = requests.get(f\"{BLOCKSTREAM_API}/block-height/{block_height}\").text.strip()\n",
    "        logger.info(f\"âœ… Got block hash: {block_hash}\")\n",
    "\n",
    "        block_data = requests.get(f\"{BLOCKSTREAM_API}/block/{block_hash}\").json()\n",
    "\n",
    "        txs = requests.get(f\"{BLOCKSTREAM_API}/block/{block_hash}/txs\").json()\n",
    "        logger.info(f\"ðŸ“¦ Found {len(txs)} transactions in block {block_height} (first page)\")\n",
    "\n",
    "        detailed_txs = []\n",
    "        for tx in txs:\n",
    "            txid = tx[\"txid\"]\n",
    "            try:\n",
    "                detailed_tx = requests.get(f\"{BLOCKSTREAM_API}/tx/{txid}\").json()\n",
    "                detailed_txs.append(detailed_tx)\n",
    "            except Exception as e:\n",
    "                logger.error(f\"âš ï¸ Failed to fetch tx {txid}: {e}\")\n",
    "                logger.error(traceback.format_exc())\n",
    "\n",
    "        logger.info(f\"âœ… Extracted {len(detailed_txs)} detailed transactions\")\n",
    "        return block_data, detailed_txs\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"âŒ Extract step failed: {e}\")\n",
    "        logger.error(traceback.format_exc())\n",
    "        raise\n",
    "\n",
    "\n",
    "# ------------------------------\n",
    "# TRANSFORM\n",
    "# ------------------------------\n",
    "def transform_block(block_data, detailed_txs):\n",
    "    try:\n",
    "        logger.info(\"ðŸ”„ Transforming data...\")\n",
    "\n",
    "        block_df = pd.DataFrame([{\n",
    "            \"block_id\": block_data[\"id\"],\n",
    "            \"height\": block_data[\"height\"],\n",
    "            \"timestamp\": pd.to_datetime(block_data[\"timestamp\"], unit=\"s\"),\n",
    "            \"tx_count\": block_data[\"tx_count\"],\n",
    "            \"size\": block_data[\"size\"],\n",
    "            \"weight\": block_data[\"weight\"]\n",
    "        }])\n",
    "\n",
    "        tx_list, input_list, output_list = [], [], []\n",
    "        address_dict = {}\n",
    "\n",
    "        for tx in detailed_txs:\n",
    "            tx_time = pd.to_datetime(tx.get(\"status\", {}).get(\"block_time\"), unit=\"s\")\n",
    "\n",
    "            tx_list.append({\n",
    "                \"txid\": tx[\"txid\"],\n",
    "                \"block_id\": block_data[\"id\"],\n",
    "                \"fee\": tx.get(\"fee\"),\n",
    "                \"size\": tx.get(\"size\"),\n",
    "                \"weight\": tx.get(\"weight\"),\n",
    "                \"version\": tx.get(\"version\"),\n",
    "                \"locktime\": tx.get(\"locktime\")\n",
    "            })\n",
    "\n",
    "            # Inputs\n",
    "            for vin in tx.get(\"vin\", []):\n",
    "                prevout = vin.get(\"prevout\") or {}\n",
    "                input_list.append({\n",
    "                    \"txid\": tx[\"txid\"],\n",
    "                    \"prev_txid\": vin.get(\"txid\"),\n",
    "                    \"prev_index\": vin.get(\"vout\"),\n",
    "                    \"input_address\": prevout.get(\"scriptpubkey_address\"),\n",
    "                    \"input_value\": prevout.get(\"value\")\n",
    "                })\n",
    "\n",
    "                addr = prevout.get(\"scriptpubkey_address\")\n",
    "                if addr:\n",
    "                    address_dict.setdefault(addr, {\n",
    "                        \"total_received\": 0, \"total_sent\": 0,\n",
    "                        \"first_seen\": tx_time, \"last_seen\": tx_time\n",
    "                    })\n",
    "                    address_dict[addr][\"total_sent\"] += prevout.get(\"value\", 0)\n",
    "                    address_dict[addr][\"last_seen\"] = tx_time\n",
    "\n",
    "            # Outputs\n",
    "            for idx, vout in enumerate(tx.get(\"vout\", [])):\n",
    "                output_list.append({\n",
    "                    \"txid\": tx[\"txid\"],\n",
    "                    \"vout_index\": idx,\n",
    "                    \"output_address\": vout.get(\"scriptpubkey_address\"),\n",
    "                    \"output_value\": vout.get(\"value\"),\n",
    "                    \"spent\": vout.get(\"spent\", False)\n",
    "                })\n",
    "\n",
    "                addr = vout.get(\"scriptpubkey_address\")\n",
    "                if addr:\n",
    "                    address_dict.setdefault(addr, {\n",
    "                        \"total_received\": 0, \"total_sent\": 0,\n",
    "                        \"first_seen\": tx_time, \"last_seen\": tx_time\n",
    "                    })\n",
    "                    address_dict[addr][\"total_received\"] += vout.get(\"value\", 0)\n",
    "                    if address_dict[addr][\"first_seen\"] is None:\n",
    "                        address_dict[addr][\"first_seen\"] = tx_time\n",
    "                    address_dict[addr][\"last_seen\"] = tx_time\n",
    "\n",
    "        tx_df = pd.DataFrame(tx_list)\n",
    "        inputs_df = pd.DataFrame(input_list)\n",
    "        outputs_df = pd.DataFrame(output_list)\n",
    "\n",
    "        addr_list = []\n",
    "        for addr, stats in address_dict.items():\n",
    "            balance = stats[\"total_received\"] - stats[\"total_sent\"]\n",
    "            addr_list.append({\n",
    "                \"address\": addr,\n",
    "                \"total_received\": stats[\"total_received\"],\n",
    "                \"total_sent\": stats[\"total_sent\"],\n",
    "                \"balance\": balance,\n",
    "                \"first_seen\": stats[\"first_seen\"],\n",
    "                \"last_seen\": stats[\"last_seen\"]\n",
    "            })\n",
    "        address_df = pd.DataFrame(addr_list)\n",
    "\n",
    "        logger.info(f\"âœ… Transform complete: {len(tx_df)} txs, {len(inputs_df)} inputs, {len(outputs_df)} outputs, {len(address_df)} addresses\")\n",
    "        return block_df, tx_df, inputs_df, outputs_df, address_df\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"âŒ Transform step failed: {e}\")\n",
    "        logger.error(traceback.format_exc())\n",
    "        raise\n",
    "\n",
    "\n",
    "# ------------------------------\n",
    "# LOAD (same as before)\n",
    "# ------------------------------\n",
    "def load_to_postgres(block_df, tx_df, inputs_df, outputs_df, address_df):\n",
    "    try:\n",
    "        logger.info(\"ðŸ”„ Loading data into PostgreSQL...\")\n",
    "\n",
    "        conn = psycopg2.connect(\n",
    "            host=DB_HOST, port=DB_PORT,\n",
    "            dbname=DB_NAME, user=DB_USER, password=DB_PASS\n",
    "        )\n",
    "        cursor = conn.cursor()\n",
    "\n",
    "        # Create tables\n",
    "        cursor.execute(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS bitcoin_blocks (\n",
    "            block_id TEXT PRIMARY KEY,\n",
    "            height NUMERIC,\n",
    "            timestamp TIMESTAMP,\n",
    "            tx_count NUMERIC,\n",
    "            size BIGINT,\n",
    "            weight BIGINT\n",
    "        );\n",
    "        \"\"\")\n",
    "        cursor.execute(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS bitcoin_transactions (\n",
    "            txid TEXT PRIMARY KEY,\n",
    "            block_id TEXT,\n",
    "            fee NUMERIC,\n",
    "            size BIGINT,\n",
    "            weight BIGINT,\n",
    "            version NUMERIC,\n",
    "            locktime BIGINT,\n",
    "            FOREIGN KEY(block_id) REFERENCES bitcoin_blocks(block_id)\n",
    "        );\n",
    "        \"\"\")\n",
    "        cursor.execute(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS bitcoin_inputs (\n",
    "            id SERIAL PRIMARY KEY,\n",
    "            txid TEXT,\n",
    "            prev_txid TEXT,\n",
    "            prev_index NUMERIC,\n",
    "            input_address TEXT,\n",
    "            input_value NUMERIC\n",
    "        );\n",
    "        \"\"\")\n",
    "        cursor.execute(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS bitcoin_outputs (\n",
    "            id SERIAL PRIMARY KEY,\n",
    "            txid TEXT,\n",
    "            vout_index NUMERIC,\n",
    "            output_address TEXT,\n",
    "            output_value NUMERIC,\n",
    "            spent BOOLEAN\n",
    "        );\n",
    "        \"\"\")\n",
    "        cursor.execute(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS bitcoin_addresses (\n",
    "            address TEXT PRIMARY KEY,\n",
    "            total_received NUMERIC,\n",
    "            total_sent NUMERIC,\n",
    "            balance NUMERIC,\n",
    "            first_seen TIMESTAMP,\n",
    "            last_seen TIMESTAMP\n",
    "        );\n",
    "        \"\"\")\n",
    "\n",
    "        # Insert block\n",
    "        for _, row in block_df.iterrows():\n",
    "            try:\n",
    "                cursor.execute(\"\"\"\n",
    "                INSERT INTO bitcoin_blocks (block_id, height, timestamp, tx_count, size, weight)\n",
    "                VALUES (%s, %s, %s, %s, %s, %s)\n",
    "                ON CONFLICT (block_id) DO NOTHING;\n",
    "                \"\"\", (row.block_id, row.height, row.timestamp, row.tx_count, row.size, row.weight))\n",
    "            except Exception as e:\n",
    "                logger.error(f\"âŒ Failed inserting block: {e}\")\n",
    "                logger.error(traceback.format_exc())\n",
    "\n",
    "        # Insert transactions\n",
    "        logger.info(f\"Inserting {len(tx_df)} transactions...\")\n",
    "        for _, row in tx_df.iterrows():\n",
    "            try:\n",
    "                cursor.execute(\"\"\"\n",
    "                INSERT INTO bitcoin_transactions (txid, block_id, fee, size, weight, version, locktime)\n",
    "                VALUES (%s, %s, %s, %s, %s, %s, %s)\n",
    "                ON CONFLICT (txid) DO NOTHING;\n",
    "                \"\"\", (row.txid, row.block_id, row.fee, row.size, row.weight, row.version, row.locktime))\n",
    "            except Exception as e:\n",
    "                logger.error(f\"âŒ Failed inserting transaction {row.txid}: {e}\")\n",
    "                logger.error(traceback.format_exc())\n",
    "\n",
    "        # Insert inputs\n",
    "        logger.info(f\"Inserting {len(inputs_df)} inputs...\")\n",
    "        for _, row in inputs_df.iterrows():\n",
    "            try:\n",
    "                cursor.execute(\"\"\"\n",
    "                INSERT INTO bitcoin_inputs (txid, prev_txid, prev_index, input_address, input_value)\n",
    "                VALUES (%s, %s, %s, %s, %s)\n",
    "                \"\"\", (row.txid, row.prev_txid, row.prev_index, row.input_address, row.input_value))\n",
    "            except Exception as e:\n",
    "                logger.error(f\"âŒ Failed inserting input for tx {row.txid}: {e}\")\n",
    "                logger.error(traceback.format_exc())\n",
    "\n",
    "        # Insert outputs\n",
    "        logger.info(f\"Inserting {len(outputs_df)} outputs...\")\n",
    "        for _, row in outputs_df.iterrows():\n",
    "            try:\n",
    "                cursor.execute(\"\"\"\n",
    "                INSERT INTO bitcoin_outputs (txid, vout_index, output_address, output_value, spent)\n",
    "                VALUES (%s, %s, %s, %s, %s)\n",
    "                \"\"\", (row.txid, row.vout_index, row.output_address, row.output_value, row.spent))\n",
    "            except Exception as e:\n",
    "                logger.error(f\"âŒ Failed inserting output for tx {row.txid}: {e}\")\n",
    "                logger.error(traceback.format_exc())\n",
    "\n",
    "        # Insert/update addresses\n",
    "        logger.info(f\"Upserting {len(address_df)} addresses...\")\n",
    "        for _, row in address_df.iterrows():\n",
    "            try:\n",
    "                cursor.execute(\"\"\"\n",
    "                INSERT INTO bitcoin_addresses (address, total_received, total_sent, balance, first_seen, last_seen)\n",
    "                VALUES (%s, %s, %s, %s, %s, %s)\n",
    "                ON CONFLICT (address) \n",
    "                DO UPDATE SET total_received = bitcoin_addresses.total_received + EXCLUDED.total_received,\n",
    "                              total_sent = bitcoin_addresses.total_sent + EXCLUDED.total_sent,\n",
    "                              balance = bitcoin_addresses.balance + EXCLUDED.balance,\n",
    "                              last_seen = GREATEST(bitcoin_addresses.last_seen, EXCLUDED.last_seen);\n",
    "                \"\"\", (row.address, row.total_received, row.total_sent, row.balance, row.first_seen, row.last_seen))\n",
    "            except Exception as e:\n",
    "                logger.error(f\"âŒ Failed upserting address {row.address}: {e}\")\n",
    "                logger.error(traceback.format_exc())\n",
    "\n",
    "        conn.commit()\n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "        logger.info(\"âœ… Load completed successfully.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"âŒ Load step failed: {e}\")\n",
    "        logger.error(traceback.format_exc())\n",
    "        raise\n",
    "\n",
    "\n",
    "# ------------------------------\n",
    "# STREAMING WITH CHECKPOINT\n",
    "# ------------------------------\n",
    "# def stream_blocks(poll_interval=30):\n",
    "#     \"\"\"Continuously fetch new blocks from last checkpoint\"\"\"\n",
    "#     ensure_state_table()\n",
    "#     last_height = get_last_processed_height()\n",
    "\n",
    "#     if last_height:\n",
    "#         logger.info(f\"ðŸ“Œ Resuming from checkpoint block {last_height}\")\n",
    "#     else:\n",
    "#         last_height = get_latest_height()\n",
    "#         logger.info(f\"ðŸš€ Starting fresh from tip block {last_height}\")\n",
    "#         update_last_processed_height(last_height)\n",
    "\n",
    "#     while True:\n",
    "#         tip = get_latest_height()\n",
    "#         if last_height < tip:\n",
    "#             for h in range(last_height + 1, tip + 1):\n",
    "#                 logger.info(f\"ðŸ“¦ Processing block {h}\")\n",
    "#                 block_data, detailed_txs = extract_block(h)\n",
    "#                 block_df, tx_df, inputs_df, outputs_df, address_df = transform_block(block_data, detailed_txs)\n",
    "#                 load_to_postgres(block_df, tx_df, inputs_df, outputs_df, address_df)\n",
    "#                 update_last_processed_height(h)\n",
    "#                 last_height = h\n",
    "#         else:\n",
    "#             logger.info(\"â³ No new block yet...\")\n",
    "#         time.sleep(poll_interval)\n",
    "\n",
    "# ------------------------------\n",
    "# STREAMING WITH CHECKPOINT\n",
    "# ------------------------------\n",
    "#START_HEIGHT = 80000  # <--- your chosen starting block\n",
    "\n",
    "def stream_blocks(poll_interval=30):\n",
    "    \"\"\"Continuously fetch new blocks from last checkpoint, start height, or start date\"\"\"\n",
    "    ensure_state_table()\n",
    "    last_height = get_last_processed_height()\n",
    "\n",
    "    if last_height:\n",
    "        logger.info(f\"ðŸ“Œ Resuming from checkpoint block {last_height}\")\n",
    "    else:\n",
    "        if START_HEIGHT:\n",
    "            last_height = START_HEIGHT\n",
    "            logger.info(f\"ðŸš€ Starting from configured block height {START_HEIGHT}\")\n",
    "        elif START_DATE:\n",
    "            last_height = get_height_from_date(START_DATE)\n",
    "            logger.info(f\"ðŸš€ Starting from first block of date {START_DATE} (height {last_height})\")\n",
    "        else:\n",
    "            last_height = get_latest_height()\n",
    "            logger.info(f\"ðŸš€ Starting fresh from tip block {last_height}\")\n",
    "\n",
    "        update_last_processed_height(last_height)\n",
    "\n",
    "    while True:\n",
    "        tip = get_latest_height()\n",
    "        if last_height < tip:\n",
    "            for h in range(last_height + 1, tip + 1):\n",
    "                logger.info(f\"ðŸ“¦ Processing block {h}\")\n",
    "                block_data, detailed_txs = extract_block(h)\n",
    "                block_df, tx_df, inputs_df, outputs_df, address_df = transform_block(block_data, detailed_txs)\n",
    "                load_to_postgres(block_df, tx_df, inputs_df, outputs_df, address_df)\n",
    "                update_last_processed_height(h)\n",
    "                last_height = h\n",
    "        else:\n",
    "            logger.info(\"â³ No new block yet...\")\n",
    "        time.sleep(poll_interval)\n",
    "\n",
    "# ------------------------------\n",
    "# MAIN\n",
    "# ------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        stream_blocks(poll_interval=30)\n",
    "    except Exception as e:\n",
    "        logger.critical(f\"ðŸš¨ ETL pipeline crashed: {e}\")\n",
    "        logger.critical(traceback.format_exc())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d733b67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e283d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import psycopg2\n",
    "import pandas as pd\n",
    "import logging\n",
    "import sys\n",
    "import traceback\n",
    "import time\n",
    "\n",
    "# ------------------------------\n",
    "# CONFIGURATION\n",
    "# ------------------------------\n",
    "BLOCKSTREAM_API = \"https://blockstream.info/api\"\n",
    "DB_HOST = \"localhost\"\n",
    "DB_PORT = \"5432\"\n",
    "DB_NAME = \"bitcoin_db\"\n",
    "DB_USER = \"postgres\"\n",
    "DB_PASS = \"chibuoke3456\"\n",
    "\n",
    "START_HEIGHT = None   # set to an int like 80000 if you want height-based start\n",
    "START_DATE = \"2021-01-01\"  # set to a YYYY-MM-DD string if you want date-based start\n",
    "\n",
    "# ------------------------------\n",
    "# LOGGING SETUP\n",
    "# ------------------------------\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s [%(levelname)s] %(message)s\",\n",
    "    handlers=[\n",
    "        logging.StreamHandler(sys.stdout),\n",
    "        logging.FileHandler(\"etl_bitcoin.log\", mode=\"a\")\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "# ------------------------------\n",
    "# DB HELPERS\n",
    "# ------------------------------\n",
    "def get_connection():\n",
    "    return psycopg2.connect(\n",
    "        host=DB_HOST, port=DB_PORT,\n",
    "        dbname=DB_NAME, user=DB_USER, password=DB_PASS\n",
    "    )\n",
    "\n",
    "\n",
    "def ensure_state_table():\n",
    "    \"\"\"Create table to track last processed block height\"\"\"\n",
    "    conn = get_connection()\n",
    "    cur = conn.cursor()\n",
    "    cur.execute(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS etl_state (\n",
    "        id SERIAL PRIMARY KEY,\n",
    "        last_block_height BIGINT\n",
    "    );\n",
    "    \"\"\")\n",
    "    conn.commit()\n",
    "    cur.close()\n",
    "    conn.close()\n",
    "\n",
    "\n",
    "def get_last_processed_height():\n",
    "    \"\"\"Fetch last processed block height from Postgres\"\"\"\n",
    "    conn = get_connection()\n",
    "    cur = conn.cursor()\n",
    "    cur.execute(\"SELECT last_block_height FROM etl_state ORDER BY id DESC LIMIT 1;\")\n",
    "    row = cur.fetchone()\n",
    "    cur.close()\n",
    "    conn.close()\n",
    "    return row[0] if row else None\n",
    "\n",
    "\n",
    "def update_last_processed_height(height: int):\n",
    "    \"\"\"Update last processed block height\"\"\"\n",
    "    conn = get_connection()\n",
    "    cur = conn.cursor()\n",
    "    cur.execute(\"INSERT INTO etl_state (last_block_height) VALUES (%s);\", (height,))\n",
    "    conn.commit()\n",
    "    cur.close()\n",
    "    conn.close()\n",
    "    logger.info(f\"âœ… Updated checkpoint: last_block_height = {height}\")\n",
    "\n",
    "\n",
    "\n",
    "#------------------------------\n",
    "#HELPER FUNCTION TO FIND START HEIGHT BASED ON DATE\n",
    "#------------------------------\n",
    "\n",
    "def get_height_from_date(date_str: str):\n",
    "    \"\"\"\n",
    "    Get the first block height from a given date (YYYY-MM-DD).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        url = f\"{BLOCKSTREAM_API}/blocks/{date_str}\"\n",
    "        resp = requests.get(url).json()\n",
    "        if not resp:\n",
    "            raise Exception(f\"No blocks found for {date_str}\")\n",
    "        # take the earliest block that day\n",
    "        block = resp[-1]  # API returns newest first, so last one is earliest\n",
    "        logger.info(f\"ðŸ“… Using block {block['height']} from {date_str}\")\n",
    "        return block[\"height\"]\n",
    "    except Exception as e:\n",
    "        logger.error(f\"âŒ Failed to fetch block height for date {date_str}: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "# ------------------------------\n",
    "# GET LATEST BLOCK HEIGHT\n",
    "# ------------------------------\n",
    "def get_latest_height():\n",
    "    return int(requests.get(f\"{BLOCKSTREAM_API}/blocks/tip/height\").text.strip())\n",
    "\n",
    "\n",
    "# ------------------------------\n",
    "# EXTRACT\n",
    "# ------------------------------\n",
    "def extract_block(block_height: int):\n",
    "    try:\n",
    "        logger.info(f\"ðŸ”„ Extracting block {block_height}\")\n",
    "\n",
    "        block_hash = requests.get(f\"{BLOCKSTREAM_API}/block-height/{block_height}\").text.strip()\n",
    "        logger.info(f\"âœ… Got block hash: {block_hash}\")\n",
    "\n",
    "        block_data = requests.get(f\"{BLOCKSTREAM_API}/block/{block_hash}\").json()\n",
    "\n",
    "        txs = requests.get(f\"{BLOCKSTREAM_API}/block/{block_hash}/txs\").json()\n",
    "        logger.info(f\"ðŸ“¦ Found {len(txs)} transactions in block {block_height} (first page)\")\n",
    "\n",
    "        detailed_txs = []\n",
    "        for tx in txs:\n",
    "            txid = tx[\"txid\"]\n",
    "            try:\n",
    "                detailed_tx = requests.get(f\"{BLOCKSTREAM_API}/tx/{txid}\").json()\n",
    "                detailed_txs.append(detailed_tx)\n",
    "            except Exception as e:\n",
    "                logger.error(f\"âš ï¸ Failed to fetch tx {txid}: {e}\")\n",
    "                logger.error(traceback.format_exc())\n",
    "\n",
    "        logger.info(f\"âœ… Extracted {len(detailed_txs)} detailed transactions\")\n",
    "        return block_data, detailed_txs\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"âŒ Extract step failed: {e}\")\n",
    "        logger.error(traceback.format_exc())\n",
    "        raise\n",
    "\n",
    "\n",
    "# ------------------------------\n",
    "# TRANSFORM\n",
    "# ------------------------------\n",
    "def transform_block(block_data, detailed_txs):\n",
    "    try:\n",
    "        logger.info(\"ðŸ”„ Transforming data...\")\n",
    "\n",
    "        block_df = pd.DataFrame([{\n",
    "            \"block_id\": block_data[\"id\"],\n",
    "            \"height\": block_data[\"height\"],\n",
    "            \"timestamp\": pd.to_datetime(block_data[\"timestamp\"], unit=\"s\"),\n",
    "            \"tx_count\": block_data[\"tx_count\"],\n",
    "            \"size\": block_data[\"size\"],\n",
    "            \"weight\": block_data[\"weight\"]\n",
    "        }])\n",
    "\n",
    "        tx_list, input_list, output_list = [], [], []\n",
    "        address_dict = {}\n",
    "\n",
    "        for tx in detailed_txs:\n",
    "            tx_time = pd.to_datetime(tx.get(\"status\", {}).get(\"block_time\"), unit=\"s\")\n",
    "\n",
    "            tx_list.append({\n",
    "                \"txid\": tx[\"txid\"],\n",
    "                \"block_id\": block_data[\"id\"],\n",
    "                \"fee\": tx.get(\"fee\"),\n",
    "                \"size\": tx.get(\"size\"),\n",
    "                \"weight\": tx.get(\"weight\"),\n",
    "                \"version\": tx.get(\"version\"),\n",
    "                \"locktime\": tx.get(\"locktime\")\n",
    "            })\n",
    "\n",
    "            # Inputs\n",
    "            for vin in tx.get(\"vin\", []):\n",
    "                prevout = vin.get(\"prevout\") or {}\n",
    "                input_list.append({\n",
    "                    \"txid\": tx[\"txid\"],\n",
    "                    \"prev_txid\": vin.get(\"txid\"),\n",
    "                    \"prev_index\": vin.get(\"vout\"),\n",
    "                    \"input_address\": prevout.get(\"scriptpubkey_address\"),\n",
    "                    \"input_value\": prevout.get(\"value\")\n",
    "                })\n",
    "\n",
    "                addr = prevout.get(\"scriptpubkey_address\")\n",
    "                if addr:\n",
    "                    address_dict.setdefault(addr, {\n",
    "                        \"total_received\": 0, \"total_sent\": 0,\n",
    "                        \"first_seen\": tx_time, \"last_seen\": tx_time\n",
    "                    })\n",
    "                    address_dict[addr][\"total_sent\"] += prevout.get(\"value\", 0)\n",
    "                    address_dict[addr][\"last_seen\"] = tx_time\n",
    "\n",
    "            # Outputs\n",
    "            for idx, vout in enumerate(tx.get(\"vout\", [])):\n",
    "                output_list.append({\n",
    "                    \"txid\": tx[\"txid\"],\n",
    "                    \"vout_index\": idx,\n",
    "                    \"output_address\": vout.get(\"scriptpubkey_address\"),\n",
    "                    \"output_value\": vout.get(\"value\"),\n",
    "                    \"spent\": vout.get(\"spent\", False)\n",
    "                })\n",
    "\n",
    "                addr = vout.get(\"scriptpubkey_address\")\n",
    "                if addr:\n",
    "                    address_dict.setdefault(addr, {\n",
    "                        \"total_received\": 0, \"total_sent\": 0,\n",
    "                        \"first_seen\": tx_time, \"last_seen\": tx_time\n",
    "                    })\n",
    "                    address_dict[addr][\"total_received\"] += vout.get(\"value\", 0)\n",
    "                    if address_dict[addr][\"first_seen\"] is None:\n",
    "                        address_dict[addr][\"first_seen\"] = tx_time\n",
    "                    address_dict[addr][\"last_seen\"] = tx_time\n",
    "\n",
    "        tx_df = pd.DataFrame(tx_list)\n",
    "        inputs_df = pd.DataFrame(input_list)\n",
    "        outputs_df = pd.DataFrame(output_list)\n",
    "\n",
    "        addr_list = []\n",
    "        for addr, stats in address_dict.items():\n",
    "            balance = stats[\"total_received\"] - stats[\"total_sent\"]\n",
    "            addr_list.append({\n",
    "                \"address\": addr,\n",
    "                \"total_received\": stats[\"total_received\"],\n",
    "                \"total_sent\": stats[\"total_sent\"],\n",
    "                \"balance\": balance,\n",
    "                \"first_seen\": stats[\"first_seen\"],\n",
    "                \"last_seen\": stats[\"last_seen\"]\n",
    "            })\n",
    "        address_df = pd.DataFrame(addr_list)\n",
    "\n",
    "        logger.info(f\"âœ… Transform complete: {len(tx_df)} txs, {len(inputs_df)} inputs, {len(outputs_df)} outputs, {len(address_df)} addresses\")\n",
    "        return block_df, tx_df, inputs_df, outputs_df, address_df\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"âŒ Transform step failed: {e}\")\n",
    "        logger.error(traceback.format_exc())\n",
    "        raise\n",
    "\n",
    "\n",
    "# ------------------------------\n",
    "# LOAD (same as before)\n",
    "# ------------------------------\n",
    "def load_to_postgres(block_df, tx_df, inputs_df, outputs_df, address_df):\n",
    "    try:\n",
    "        logger.info(\"ðŸ”„ Loading data into PostgreSQL...\")\n",
    "\n",
    "        conn = psycopg2.connect(\n",
    "            host=DB_HOST, port=DB_PORT,\n",
    "            dbname=DB_NAME, user=DB_USER, password=DB_PASS\n",
    "        )\n",
    "        cursor = conn.cursor()\n",
    "\n",
    "        # Create tables\n",
    "        cursor.execute(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS bitcoin_blocks (\n",
    "            block_id TEXT PRIMARY KEY,\n",
    "            height NUMERIC,\n",
    "            timestamp TIMESTAMP,\n",
    "            tx_count NUMERIC,\n",
    "            size BIGINT,\n",
    "            weight BIGINT\n",
    "        );\n",
    "        \"\"\")\n",
    "        cursor.execute(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS bitcoin_transactions (\n",
    "            txid TEXT PRIMARY KEY,\n",
    "            block_id TEXT,\n",
    "            fee NUMERIC,\n",
    "            size BIGINT,\n",
    "            weight BIGINT,\n",
    "            version NUMERIC,\n",
    "            locktime BIGINT,\n",
    "            FOREIGN KEY(block_id) REFERENCES bitcoin_blocks(block_id)\n",
    "        );\n",
    "        \"\"\")\n",
    "        cursor.execute(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS bitcoin_inputs (\n",
    "            id SERIAL PRIMARY KEY,\n",
    "            txid TEXT,\n",
    "            prev_txid TEXT,\n",
    "            prev_index NUMERIC,\n",
    "            input_address TEXT,\n",
    "            input_value NUMERIC\n",
    "        );\n",
    "        \"\"\")\n",
    "        cursor.execute(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS bitcoin_outputs (\n",
    "            id SERIAL PRIMARY KEY,\n",
    "            txid TEXT,\n",
    "            vout_index NUMERIC,\n",
    "            output_address TEXT,\n",
    "            output_value NUMERIC,\n",
    "            spent BOOLEAN\n",
    "        );\n",
    "        \"\"\")\n",
    "        cursor.execute(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS bitcoin_addresses (\n",
    "            address TEXT PRIMARY KEY,\n",
    "            total_received NUMERIC,\n",
    "            total_sent NUMERIC,\n",
    "            balance NUMERIC,\n",
    "            first_seen TIMESTAMP,\n",
    "            last_seen TIMESTAMP\n",
    "        );\n",
    "        \"\"\")\n",
    "\n",
    "        # Insert block\n",
    "        for _, row in block_df.iterrows():\n",
    "            try:\n",
    "                cursor.execute(\"\"\"\n",
    "                INSERT INTO bitcoin_blocks (block_id, height, timestamp, tx_count, size, weight)\n",
    "                VALUES (%s, %s, %s, %s, %s, %s)\n",
    "                ON CONFLICT (block_id) DO NOTHING;\n",
    "                \"\"\", (row.block_id, row.height, row.timestamp, row.tx_count, row.size, row.weight))\n",
    "            except Exception as e:\n",
    "                logger.error(f\"âŒ Failed inserting block: {e}\")\n",
    "                logger.error(traceback.format_exc())\n",
    "\n",
    "        # Insert transactions\n",
    "        logger.info(f\"Inserting {len(tx_df)} transactions...\")\n",
    "        for _, row in tx_df.iterrows():\n",
    "            try:\n",
    "                cursor.execute(\"\"\"\n",
    "                INSERT INTO bitcoin_transactions (txid, block_id, fee, size, weight, version, locktime)\n",
    "                VALUES (%s, %s, %s, %s, %s, %s, %s)\n",
    "                ON CONFLICT (txid) DO NOTHING;\n",
    "                \"\"\", (row.txid, row.block_id, row.fee, row.size, row.weight, row.version, row.locktime))\n",
    "            except Exception as e:\n",
    "                logger.error(f\"âŒ Failed inserting transaction {row.txid}: {e}\")\n",
    "                logger.error(traceback.format_exc())\n",
    "\n",
    "        # Insert inputs\n",
    "        logger.info(f\"Inserting {len(inputs_df)} inputs...\")\n",
    "        for _, row in inputs_df.iterrows():\n",
    "            try:\n",
    "                cursor.execute(\"\"\"\n",
    "                INSERT INTO bitcoin_inputs (txid, prev_txid, prev_index, input_address, input_value)\n",
    "                VALUES (%s, %s, %s, %s, %s)\n",
    "                \"\"\", (row.txid, row.prev_txid, row.prev_index, row.input_address, row.input_value))\n",
    "            except Exception as e:\n",
    "                logger.error(f\"âŒ Failed inserting input for tx {row.txid}: {e}\")\n",
    "                logger.error(traceback.format_exc())\n",
    "\n",
    "        # Insert outputs\n",
    "        logger.info(f\"Inserting {len(outputs_df)} outputs...\")\n",
    "        for _, row in outputs_df.iterrows():\n",
    "            try:\n",
    "                cursor.execute(\"\"\"\n",
    "                INSERT INTO bitcoin_outputs (txid, vout_index, output_address, output_value, spent)\n",
    "                VALUES (%s, %s, %s, %s, %s)\n",
    "                \"\"\", (row.txid, row.vout_index, row.output_address, row.output_value, row.spent))\n",
    "            except Exception as e:\n",
    "                logger.error(f\"âŒ Failed inserting output for tx {row.txid}: {e}\")\n",
    "                logger.error(traceback.format_exc())\n",
    "\n",
    "        # Insert/update addresses\n",
    "        logger.info(f\"Upserting {len(address_df)} addresses...\")\n",
    "        for _, row in address_df.iterrows():\n",
    "            try:\n",
    "                cursor.execute(\"\"\"\n",
    "                INSERT INTO bitcoin_addresses (address, total_received, total_sent, balance, first_seen, last_seen)\n",
    "                VALUES (%s, %s, %s, %s, %s, %s)\n",
    "                ON CONFLICT (address) \n",
    "                DO UPDATE SET total_received = bitcoin_addresses.total_received + EXCLUDED.total_received,\n",
    "                              total_sent = bitcoin_addresses.total_sent + EXCLUDED.total_sent,\n",
    "                              balance = bitcoin_addresses.balance + EXCLUDED.balance,\n",
    "                              last_seen = GREATEST(bitcoin_addresses.last_seen, EXCLUDED.last_seen);\n",
    "                \"\"\", (row.address, row.total_received, row.total_sent, row.balance, row.first_seen, row.last_seen))\n",
    "            except Exception as e:\n",
    "                logger.error(f\"âŒ Failed upserting address {row.address}: {e}\")\n",
    "                logger.error(traceback.format_exc())\n",
    "\n",
    "        conn.commit()\n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "        logger.info(\"âœ… Load completed successfully.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"âŒ Load step failed: {e}\")\n",
    "        logger.error(traceback.format_exc())\n",
    "        raise\n",
    "\n",
    "\n",
    "\n",
    "def update_block_stats(conn, block_height):\n",
    "    with conn.cursor() as cur:\n",
    "        # Ensure mart exists\n",
    "        cur.execute(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS datamart.mart_block_stats (\n",
    "            height INT PRIMARY KEY,\n",
    "            block_date DATE,\n",
    "            tx_count BIGINT,\n",
    "            avg_fee NUMERIC,\n",
    "            total_fees NUMERIC,\n",
    "            avg_tx_size NUMERIC,\n",
    "            avg_block_size NUMERIC,\n",
    "            avg_block_weight NUMERIC\n",
    "        );\n",
    "        \"\"\")\n",
    "        \n",
    "        # Incremental update for this block\n",
    "        cur.execute(\"\"\"\n",
    "        INSERT INTO datamart.mart_block_stats (height, block_date, tx_count, avg_fee, total_fees, avg_tx_size, avg_block_size, avg_block_weight)\n",
    "        SELECT \n",
    "            b.height,\n",
    "            b.timestamp::date,\n",
    "            COUNT(t.txid),\n",
    "            AVG(t.fee),\n",
    "            SUM(t.fee),\n",
    "            AVG(t.size),\n",
    "            AVG(b.size),\n",
    "            AVG(b.weight)\n",
    "        FROM bitcoin_blocks b\n",
    "        LEFT JOIN bitcoin_transactions t ON b.block_id = t.block_id\n",
    "        WHERE b.height = %s\n",
    "        GROUP BY b.height, b.timestamp::date\n",
    "        ON CONFLICT (height) DO NOTHING;\n",
    "        \"\"\", (block_height,))\n",
    "    conn.commit()\n",
    "\n",
    "\n",
    "\n",
    "def update_address_activity(conn, block_height):\n",
    "    with conn.cursor() as cur:\n",
    "        cur.execute(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS datamart.mart_address_activity (\n",
    "            address TEXT,\n",
    "            block_height INT,\n",
    "            total_received NUMERIC,\n",
    "            total_sent NUMERIC,\n",
    "            balance NUMERIC,\n",
    "            PRIMARY KEY(address, block_height)\n",
    "        );\n",
    "        \"\"\")\n",
    "        \n",
    "        cur.execute(\"\"\"\n",
    "        INSERT INTO datamart.mart_address_activity (address, block_height, total_received, total_sent, balance)\n",
    "        SELECT \n",
    "            a.address,\n",
    "            b.height,\n",
    "            a.total_received,\n",
    "            a.total_sent,\n",
    "            a.balance\n",
    "        FROM bitcoin_addresses a\n",
    "        JOIN bitcoin_outputs o ON a.address = o.output_address\n",
    "        JOIN bitcoin_transactions t ON o.txid = t.txid\n",
    "        JOIN bitcoin_blocks b ON t.block_id = b.block_id\n",
    "        WHERE b.height = %s\n",
    "        ON CONFLICT (address, block_height) DO NOTHING;\n",
    "        \"\"\", (block_height,))\n",
    "    conn.commit()\n",
    "\n",
    "\n",
    "\n",
    "def update_tx_patterns(conn, block_height):\n",
    "    with conn.cursor() as cur:\n",
    "        cur.execute(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS datamart.mart_tx_patterns (\n",
    "            block_height INT,\n",
    "            txid TEXT PRIMARY KEY,\n",
    "            input_count INT,\n",
    "            output_count INT,\n",
    "            total_input_value NUMERIC,\n",
    "            total_output_value NUMERIC\n",
    "        );\n",
    "        \"\"\")\n",
    "        \n",
    "        cur.execute(\"\"\"\n",
    "        INSERT INTO datamart.mart_tx_patterns (block_height, txid, input_count, output_count, total_input_value, total_output_value)\n",
    "        SELECT\n",
    "            b.height,\n",
    "            t.txid,\n",
    "            COUNT(DISTINCT i.id) AS input_count,\n",
    "            COUNT(DISTINCT o.id) AS output_count,\n",
    "            COALESCE(SUM(i.input_value), 0),\n",
    "            COALESCE(SUM(o.output_value), 0)\n",
    "        FROM bitcoin_transactions t\n",
    "        LEFT JOIN bitcoin_inputs i ON t.txid = i.txid\n",
    "        LEFT JOIN bitcoin_outputs o ON t.txid = o.txid\n",
    "        JOIN bitcoin_blocks b ON t.block_id = b.block_id\n",
    "        WHERE b.height = %s\n",
    "        GROUP BY b.height, t.txid\n",
    "        ON CONFLICT (txid) DO NOTHING;\n",
    "        \"\"\", (block_height,))\n",
    "    conn.commit()\n",
    "\n",
    "\n",
    "\n",
    "def update_daily_tx_volume(conn, block_height):\n",
    "    with conn.cursor() as cur:\n",
    "        cur.execute(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS datamart.mart_daily_tx_volume (\n",
    "            block_date DATE PRIMARY KEY,\n",
    "            tx_count BIGINT,\n",
    "            total_fees NUMERIC,\n",
    "            total_output_value NUMERIC\n",
    "        );\n",
    "        \"\"\")\n",
    "        \n",
    "        cur.execute(\"\"\"\n",
    "        INSERT INTO datamart.mart_daily_tx_volume (block_date, tx_count, total_fees, total_output_value)\n",
    "        SELECT \n",
    "            b.timestamp::date,\n",
    "            COUNT(t.txid),\n",
    "            SUM(t.fee),\n",
    "            SUM(o.output_value)\n",
    "        FROM bitcoin_blocks b\n",
    "        LEFT JOIN bitcoin_transactions t ON b.block_id = t.block_id\n",
    "        LEFT JOIN bitcoin_outputs o ON t.txid = o.txid\n",
    "        WHERE b.height = %s\n",
    "        GROUP BY b.timestamp::date\n",
    "        ON CONFLICT (block_date) DO UPDATE \n",
    "        SET tx_count = EXCLUDED.tx_count,\n",
    "            total_fees = EXCLUDED.total_fees,\n",
    "            total_output_value = EXCLUDED.total_output_value;\n",
    "        \"\"\", (block_height,))\n",
    "    conn.commit()\n",
    "\n",
    "def update_whale_activity(conn, block_height):\n",
    "    with conn.cursor() as cur:\n",
    "        # Ensure mart exists\n",
    "        cur.execute(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS datamart.mart_whale_activity (\n",
    "            block_height INT PRIMARY KEY,\n",
    "            block_date DATE,\n",
    "            whale_tx_count BIGINT,\n",
    "            whale_volume NUMERIC\n",
    "        );\n",
    "        \"\"\")\n",
    "        \n",
    "        # Incremental insert for whale activity (â‰¥ 100 BTC)\n",
    "        cur.execute(\"\"\"\n",
    "        INSERT INTO datamart.mart_whale_activity (\n",
    "            block_height, block_date, whale_tx_count, whale_volume\n",
    "        )\n",
    "        SELECT \n",
    "            b.height,\n",
    "            b.timestamp::date,\n",
    "            COUNT(DISTINCT t.txid),\n",
    "            SUM(o.output_value)\n",
    "        FROM bitcoin_blocks b\n",
    "        JOIN bitcoin_transactions t ON b.block_id = t.block_id\n",
    "        JOIN bitcoin_outputs o ON t.txid = o.txid\n",
    "        WHERE b.height = %s\n",
    "          AND o.output_value >= 100*100000000  -- 100 BTC in satoshis\n",
    "        GROUP BY b.height, b.timestamp::date\n",
    "        ON CONFLICT (block_height) DO NOTHING;\n",
    "        \"\"\", (block_height,))\n",
    "    conn.commit()\n",
    "\n",
    "def update_marts(conn, block_height):\n",
    "    \"\"\"Call all mart updates for a given block\"\"\"\n",
    "    update_block_stats(conn, block_height)\n",
    "    update_address_activity(conn, block_height)\n",
    "    update_tx_patterns(conn, block_height)\n",
    "    update_daily_tx_volume(conn, block_height)\n",
    "    update_whale_activity(conn, block_height)\n",
    "    logger.info(f\"âœ… Updated data marts for block {block_height}\")\n",
    "# ------------------------------\n",
    "# STREAMING WITH CHECKPOINT\n",
    "# ------------------------------\n",
    "# def stream_blocks(poll_interval=30):\n",
    "#     \"\"\"Continuously fetch new blocks from last checkpoint\"\"\"\n",
    "#     ensure_state_table()\n",
    "#     last_height = get_last_processed_height()\n",
    "\n",
    "#     if last_height:\n",
    "#         logger.info(f\"ðŸ“Œ Resuming from checkpoint block {last_height}\")\n",
    "#     else:\n",
    "#         last_height = get_latest_height()\n",
    "#         logger.info(f\"ðŸš€ Starting fresh from tip block {last_height}\")\n",
    "#         update_last_processed_height(last_height)\n",
    "\n",
    "#     while True:\n",
    "#         tip = get_latest_height()\n",
    "#         if last_height < tip:\n",
    "#             for h in range(last_height + 1, tip + 1):\n",
    "#                 logger.info(f\"ðŸ“¦ Processing block {h}\")\n",
    "#                 block_data, detailed_txs = extract_block(h)\n",
    "#                 block_df, tx_df, inputs_df, outputs_df, address_df = transform_block(block_data, detailed_txs)\n",
    "#                 load_to_postgres(block_df, tx_df, inputs_df, outputs_df, address_df)\n",
    "#                 update_last_processed_height(h)\n",
    "#                 last_height = h\n",
    "#         else:\n",
    "#             logger.info(\"â³ No new block yet...\")\n",
    "#         time.sleep(poll_interval)\n",
    "\n",
    "# ------------------------------\n",
    "# STREAMING WITH CHECKPOINT\n",
    "# ------------------------------\n",
    "#START_HEIGHT = 80000  # <--- your chosen starting block\n",
    "\n",
    "def stream_blocks(poll_interval=30):\n",
    "    \"\"\"Continuously fetch new blocks from last checkpoint, start height, or start date\"\"\"\n",
    "    ensure_state_table()\n",
    "    last_height = get_last_processed_height()\n",
    "\n",
    "    if last_height:\n",
    "        logger.info(f\"ðŸ“Œ Resuming from checkpoint block {last_height}\")\n",
    "    else:\n",
    "        if START_HEIGHT:\n",
    "            last_height = START_HEIGHT\n",
    "            logger.info(f\"ðŸš€ Starting from configured block height {START_HEIGHT}\")\n",
    "        elif START_DATE:\n",
    "            last_height = get_height_from_date(START_DATE)\n",
    "            logger.info(f\"ðŸš€ Starting from first block of date {START_DATE} (height {last_height})\")\n",
    "        else:\n",
    "            last_height = get_latest_height()\n",
    "            logger.info(f\"ðŸš€ Starting fresh from tip block {last_height}\")\n",
    "\n",
    "        update_last_processed_height(last_height)\n",
    "\n",
    "    while True:\n",
    "        tip = get_latest_height()\n",
    "        if last_height < tip:\n",
    "            for h in range(last_height + 1, tip + 1):\n",
    "                logger.info(f\"ðŸ“¦ Processing block {h}\")\n",
    "                block_data, detailed_txs = extract_block(h)\n",
    "                block_df, tx_df, inputs_df, outputs_df, address_df = transform_block(block_data, detailed_txs)\n",
    "                load_to_postgres(block_df, tx_df, inputs_df, outputs_df, address_df)\n",
    "\n",
    "                # NEW STEP: update marts\n",
    "                conn = get_connection()\n",
    "                update_marts(conn, h)\n",
    "                conn.close()\n",
    "\n",
    "                update_last_processed_height(h)\n",
    "                last_height = h\n",
    "\n",
    "        else:\n",
    "            logger.info(\"â³ No new block yet...\")\n",
    "        time.sleep(poll_interval)\n",
    "\n",
    "# ------------------------------\n",
    "# MAIN\n",
    "# ------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        stream_blocks(poll_interval=30)\n",
    "    except Exception as e:\n",
    "        logger.critical(f\"ðŸš¨ ETL pipeline crashed: {e}\")\n",
    "        logger.critical(traceback.format_exc())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b919b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f9d026",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "Real-time Bitcoin ETL (Blockstream API) with incremental datamarts.\n",
    "\n",
    "Features:\n",
    "- Polls Blockstream API for new blocks\n",
    "- Extract -> Transform -> Load into Postgres raw tables\n",
    "- Incrementally updates datamart tables per block (created IF NOT EXISTS)\n",
    "- Uses bulk inserts (execute_batch) for performance\n",
    "- Uses a single transaction for mart updates\n",
    "- Checkpointing in `etl_state` table\n",
    "- Start from START_HEIGHT, START_DATE, or resume from checkpoint\n",
    "- Graceful shutdown (KeyboardInterrupt)\n",
    "- Robust logging (no emojis to avoid encoding errors)\n",
    "\"\"\"\n",
    "\n",
    "import requests\n",
    "import psycopg2\n",
    "from psycopg2 import sql\n",
    "from psycopg2.extras import execute_batch\n",
    "import pandas as pd\n",
    "import logging\n",
    "import sys\n",
    "import os\n",
    "import traceback\n",
    "import time\n",
    "from typing import Optional\n",
    "\n",
    "\n",
    "# CONFIGURATION\n",
    "\n",
    "BLOCKSTREAM_API = \"https://blockstream.info/api\"\n",
    "DB_HOST = os.getenv(\"DB_HOST\")\n",
    "DB_PORT = os.getenv(\"DB_PORT\")\n",
    "DB_NAME = os.getenv(\"DB_NAME\")\n",
    "DB_USER = os.getenv(\"DB_USER\")\n",
    "DB_PASS = os.getenv(\"DB_PASS\")\n",
    "\n",
    "# Choose one of:\n",
    "START_HEIGHT: Optional[int] = None    # or set a block height to start from\n",
    "START_DATE: Optional[str] = \"2021-01-01\"  # format YYYY-MM-DD or None\n",
    "\n",
    "POLL_INTERVAL = 30  # seconds between checks\n",
    "\n",
    "\n",
    "# LOGGING SETUP \n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s [%(levelname)s] %(message)s\",\n",
    "    handlers=[\n",
    "        logging.StreamHandler(sys.stdout),\n",
    "        logging.FileHandler(\"etl_bitcoin.log\", mode=\"a\", encoding=\"utf-8\")\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "# DB HELPERS\n",
    "\n",
    "def get_connection():\n",
    "    return psycopg2.connect(\n",
    "        host=DB_HOST, port=DB_PORT,\n",
    "        dbname=DB_NAME, user=DB_USER, password=DB_PASS\n",
    "    )\n",
    "\n",
    "def ensure_state_table():\n",
    "    \"\"\"Create table to track last processed block height and ensure datamart schema exists\"\"\"\n",
    "    conn = get_connection()\n",
    "    cur = conn.cursor()\n",
    "    try:\n",
    "        cur.execute(\"CREATE SCHEMA IF NOT EXISTS datamart;\")\n",
    "        cur.execute(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS etl_state (\n",
    "            id SERIAL PRIMARY KEY,\n",
    "            last_block_height BIGINT,\n",
    "            updated_at TIMESTAMP DEFAULT now()\n",
    "        );\n",
    "        \"\"\")\n",
    "        conn.commit()\n",
    "    except Exception:\n",
    "        conn.rollback()\n",
    "        logger.error(\"Failed to ensure etl_state or datamart schema exists\")\n",
    "        logger.error(traceback.format_exc())\n",
    "        raise\n",
    "    finally:\n",
    "        cur.close()\n",
    "        conn.close()\n",
    "\n",
    "def get_last_processed_height() -> Optional[int]:\n",
    "    conn = get_connection()\n",
    "    cur = conn.cursor()\n",
    "    try:\n",
    "        cur.execute(\"SELECT last_block_height FROM etl_state ORDER BY id DESC LIMIT 1;\")\n",
    "        row = cur.fetchone()\n",
    "        return int(row[0]) if row and row[0] is not None else None\n",
    "    finally:\n",
    "        cur.close()\n",
    "        conn.close()\n",
    "\n",
    "def update_last_processed_height(height: int):\n",
    "    conn = get_connection()\n",
    "    cur = conn.cursor()\n",
    "    try:\n",
    "        cur.execute(\"INSERT INTO etl_state (last_block_height) VALUES (%s);\", (height,))\n",
    "        conn.commit()\n",
    "        logger.info(f\"Updated checkpoint: last_block_height = {height}\")\n",
    "    except Exception:\n",
    "        conn.rollback()\n",
    "        logger.error(f\"Failed to update checkpoint for height {height}\")\n",
    "        logger.error(traceback.format_exc())\n",
    "        raise\n",
    "    finally:\n",
    "        cur.close()\n",
    "        conn.close()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# BLOCK HEIGHT <-> DATE helper\n",
    "\n",
    "def get_height_from_date(date_str: str) -> int:\n",
    "    \"\"\"\n",
    "    Use Blockstream endpoint /blocks/{date} to find earliest block for date.\n",
    "    Returns a block height integer.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        url = f\"{BLOCKSTREAM_API}/blocks/{date_str}\"\n",
    "        resp = requests.get(url)\n",
    "        resp.raise_for_status()\n",
    "        blocks = resp.json()\n",
    "        if not blocks:\n",
    "            raise Exception(f\"No blocks found for {date_str}\")\n",
    "        # API returns most recent first; take the last one for earliest of day\n",
    "        block = blocks[-1]\n",
    "        height = int(block[\"height\"])\n",
    "        logger.info(f\"Using block {height} for date {date_str}\")\n",
    "        return height\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to fetch block height for date {date_str}: {e}\")\n",
    "        logger.error(traceback.format_exc())\n",
    "        raise\n",
    "\n",
    "\n",
    "# GET LATEST BLOCK HEIGHT\n",
    "def get_latest_height() -> int:\n",
    "    resp = requests.get(f\"{BLOCKSTREAM_API}/blocks/tip/height\")\n",
    "    resp.raise_for_status()\n",
    "    return int(resp.text.strip())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ETL\n",
    "\n",
    "# EXTRACT\n",
    "\n",
    "def extract_block(block_height: int):\n",
    "    try:\n",
    "        logger.info(f\"Extracting block {block_height}\")\n",
    "        r = requests.get(f\"{BLOCKSTREAM_API}/block-height/{block_height}\")\n",
    "        r.raise_for_status()\n",
    "        block_hash = r.text.strip()\n",
    "        logger.info(f\"Got block hash: {block_hash}\")\n",
    "\n",
    "        \n",
    "        r2 = requests.get(f\"{BLOCKSTREAM_API}/block/{block_hash}\")\n",
    "        r2.raise_for_status()\n",
    "        block_data = r2.json()\n",
    "\n",
    "\n",
    "        r3 = requests.get(f\"{BLOCKSTREAM_API}/block/{block_hash}/txs\")\n",
    "        r3.raise_for_status()\n",
    "        txs = r3.json()\n",
    "        logger.info(f\"Found {len(txs)} transactions in block {block_height} (first page)\")\n",
    "\n",
    "        detailed_txs = []\n",
    "        for tx in txs:\n",
    "            txid = tx.get(\"txid\")\n",
    "            if not txid:\n",
    "                continue\n",
    "            try:\n",
    "                rtx = requests.get(f\"{BLOCKSTREAM_API}/tx/{txid}\")\n",
    "                rtx.raise_for_status()\n",
    "                detailed_txs.append(rtx.json())\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Failed to fetch tx {txid}: {e}\")\n",
    "                logger.error(traceback.format_exc())\n",
    "\n",
    "        logger.info(f\"Extracted {len(detailed_txs)} detailed transactions for block {block_height}\")\n",
    "        return block_data, detailed_txs\n",
    "\n",
    "    except Exception:\n",
    "        logger.error(\"Extract step failed\")\n",
    "        logger.error(traceback.format_exc())\n",
    "        raise\n",
    "\n",
    "\n",
    "# TRANSFORM\n",
    "\n",
    "def transform_block(block_data, detailed_txs):\n",
    "    try:\n",
    "        logger.info(\"Transforming block data\")\n",
    "        block_df = pd.DataFrame([{\n",
    "            \"block_id\": block_data.get(\"id\"),\n",
    "            \"height\": int(block_data.get(\"height\")),\n",
    "            \"timestamp\": pd.to_datetime(block_data.get(\"timestamp\"), unit=\"s\"),\n",
    "            \"tx_count\": int(block_data.get(\"tx_count\", 0)),\n",
    "            \"size\": int(block_data.get(\"size\", 0)),\n",
    "            \"weight\": int(block_data.get(\"weight\", 0))\n",
    "        }])\n",
    "\n",
    "        tx_list, input_list, output_list = [], [], []\n",
    "        address_dict = {}\n",
    "\n",
    "        for tx in detailed_txs:\n",
    "            block_time = tx.get(\"status\", {}).get(\"block_time\")\n",
    "            tx_time = pd.to_datetime(block_time, unit=\"s\") if block_time else None\n",
    "\n",
    "            tx_list.append({\n",
    "                \"txid\": tx.get(\"txid\"),\n",
    "                \"block_id\": block_data.get(\"id\"),\n",
    "                \"fee\": tx.get(\"fee\"),\n",
    "                \"size\": tx.get(\"size\"),\n",
    "                \"weight\": tx.get(\"weight\"),\n",
    "                \"version\": tx.get(\"version\"),\n",
    "                \"locktime\": tx.get(\"locktime\")\n",
    "            })\n",
    "\n",
    "            # Inputs\n",
    "            for vin in tx.get(\"vin\", []):\n",
    "                prevout = vin.get(\"prevout\") or {}\n",
    "                input_list.append({\n",
    "                    \"txid\": tx.get(\"txid\"),\n",
    "                    \"prev_txid\": vin.get(\"txid\"),\n",
    "                    \"prev_index\": vin.get(\"vout\"),\n",
    "                    \"input_address\": prevout.get(\"scriptpubkey_address\"),\n",
    "                    \"input_value\": prevout.get(\"value\")\n",
    "                })\n",
    "                addr = prevout.get(\"scriptpubkey_address\")\n",
    "                if addr:\n",
    "                    address_dict.setdefault(addr, {\n",
    "                        \"total_received\": 0, \"total_sent\": 0,\n",
    "                        \"first_seen\": tx_time, \"last_seen\": tx_time\n",
    "                    })\n",
    "                    address_dict[addr][\"total_sent\"] += prevout.get(\"value\", 0) or 0\n",
    "                    address_dict[addr][\"last_seen\"] = tx_time or address_dict[addr][\"last_seen\"]\n",
    "\n",
    "            # Outputs\n",
    "            for idx, vout in enumerate(tx.get(\"vout\", [])):\n",
    "                output_list.append({\n",
    "                    \"txid\": tx.get(\"txid\"),\n",
    "                    \"vout_index\": idx,\n",
    "                    \"output_address\": vout.get(\"scriptpubkey_address\"),\n",
    "                    \"output_value\": vout.get(\"value\"),\n",
    "                    \"spent\": vout.get(\"spent\", False)\n",
    "                })\n",
    "                addr = vout.get(\"scriptpubkey_address\")\n",
    "                if addr:\n",
    "                    address_dict.setdefault(addr, {\n",
    "                        \"total_received\": 0, \"total_sent\": 0,\n",
    "                        \"first_seen\": tx_time, \"last_seen\": tx_time\n",
    "                    })\n",
    "                    address_dict[addr][\"total_received\"] += vout.get(\"value\", 0) or 0\n",
    "                    if address_dict[addr][\"first_seen\"] is None:\n",
    "                        address_dict[addr][\"first_seen\"] = tx_time\n",
    "                    address_dict[addr][\"last_seen\"] = tx_time or address_dict[addr][\"last_seen\"]\n",
    "\n",
    "        tx_df = pd.DataFrame(tx_list) if tx_list else pd.DataFrame(columns=[\"txid\",\"block_id\",\"fee\",\"size\",\"weight\",\"version\",\"locktime\"])\n",
    "        inputs_df = pd.DataFrame(input_list) if input_list else pd.DataFrame(columns=[\"txid\",\"prev_txid\",\"prev_index\",\"input_address\",\"input_value\"])\n",
    "        outputs_df = pd.DataFrame(output_list) if output_list else pd.DataFrame(columns=[\"txid\",\"vout_index\",\"output_address\",\"output_value\",\"spent\"])\n",
    "\n",
    "        addr_list = []\n",
    "        for addr, stats in address_dict.items():\n",
    "            balance = (stats.get(\"total_received\", 0) or 0) - (stats.get(\"total_sent\", 0) or 0)\n",
    "            addr_list.append({\n",
    "                \"address\": addr,\n",
    "                \"total_received\": stats.get(\"total_received\", 0) or 0,\n",
    "                \"total_sent\": stats.get(\"total_sent\", 0) or 0,\n",
    "                \"balance\": balance,\n",
    "                \"first_seen\": stats.get(\"first_seen\"),\n",
    "                \"last_seen\": stats.get(\"last_seen\")\n",
    "            })\n",
    "        address_df = pd.DataFrame(addr_list) if addr_list else pd.DataFrame(columns=[\"address\",\"total_received\",\"total_sent\",\"balance\",\"first_seen\",\"last_seen\"])\n",
    "\n",
    "        logger.info(f\"Transform complete: {len(tx_df)} txs, {len(inputs_df)} inputs, {len(outputs_df)} outputs, {len(address_df)} addresses\")\n",
    "        return block_df, tx_df, inputs_df, outputs_df, address_df\n",
    "\n",
    "    except Exception:\n",
    "        logger.error(\"Transform step failed\")\n",
    "        logger.error(traceback.format_exc())\n",
    "        raise\n",
    "\n",
    "\n",
    "# LOAD (raw tables) using execute_batch for speed\n",
    "\n",
    "def load_to_postgres(block_df, tx_df, inputs_df, outputs_df, address_df):\n",
    "    try:\n",
    "        logger.info(\"Loading block and related records into Postgres (raw tables)\")\n",
    "        conn = get_connection()\n",
    "        cur = conn.cursor()\n",
    "\n",
    "        # Ensure raw tables exist\n",
    "        cur.execute(\"CREATE SCHEMA IF NOT EXISTS public;\")  # safe no-op\n",
    "        cur.execute(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS bitcoin_blocks (\n",
    "            block_id TEXT PRIMARY KEY,\n",
    "            height NUMERIC,\n",
    "            timestamp TIMESTAMP,\n",
    "            tx_count NUMERIC,\n",
    "            size BIGINT,\n",
    "            weight BIGINT\n",
    "        );\n",
    "        \"\"\")\n",
    "        cur.execute(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS bitcoin_transactions (\n",
    "            txid TEXT PRIMARY KEY,\n",
    "            block_id TEXT,\n",
    "            fee NUMERIC,\n",
    "            size BIGINT,\n",
    "            weight BIGINT,\n",
    "            version NUMERIC,\n",
    "            locktime BIGINT,\n",
    "            FOREIGN KEY(block_id) REFERENCES bitcoin_blocks(block_id)\n",
    "        );\n",
    "        \"\"\")\n",
    "        cur.execute(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS bitcoin_inputs (\n",
    "            id SERIAL PRIMARY KEY,\n",
    "            txid TEXT,\n",
    "            prev_txid TEXT,\n",
    "            prev_index NUMERIC,\n",
    "            input_address TEXT,\n",
    "            input_value NUMERIC\n",
    "        );\n",
    "        \"\"\")\n",
    "        cur.execute(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS bitcoin_outputs (\n",
    "            id SERIAL PRIMARY KEY,\n",
    "            txid TEXT,\n",
    "            vout_index NUMERIC,\n",
    "            output_address TEXT,\n",
    "            output_value NUMERIC,\n",
    "            spent BOOLEAN\n",
    "        );\n",
    "        \"\"\")\n",
    "        cur.execute(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS bitcoin_addresses (\n",
    "            address TEXT PRIMARY KEY,\n",
    "            total_received NUMERIC,\n",
    "            total_sent NUMERIC,\n",
    "            balance NUMERIC,\n",
    "            first_seen TIMESTAMP,\n",
    "            last_seen TIMESTAMP\n",
    "        );\n",
    "        \"\"\")\n",
    "        conn.commit()\n",
    "\n",
    "        # Insert block(s) (usually one)\n",
    "        if not block_df.empty:\n",
    "            block_rows = [\n",
    "                (row.block_id, row.height, row.timestamp.to_pydatetime() if not pd.isna(row.timestamp) else None,\n",
    "                 row.tx_count, row.size, row.weight)\n",
    "                for _, row in block_df.iterrows()\n",
    "            ]\n",
    "            execute_batch(cur,\n",
    "                \"\"\"\n",
    "                INSERT INTO bitcoin_blocks (block_id, height, timestamp, tx_count, size, weight)\n",
    "                VALUES (%s, %s, %s, %s, %s, %s)\n",
    "                ON CONFLICT (block_id) DO NOTHING;\n",
    "                \"\"\",\n",
    "                block_rows, page_size=100)\n",
    "\n",
    "        # Insert transactions\n",
    "        if not tx_df.empty:\n",
    "            tx_rows = [\n",
    "                (row.txid, row.block_id, row.fee, row.size, row.weight, row.version, row.locktime)\n",
    "                for _, row in tx_df.iterrows()\n",
    "            ]\n",
    "            execute_batch(cur,\n",
    "                \"\"\"\n",
    "                INSERT INTO bitcoin_transactions (txid, block_id, fee, size, weight, version, locktime)\n",
    "                VALUES (%s, %s, %s, %s, %s, %s, %s)\n",
    "                ON CONFLICT (txid) DO NOTHING;\n",
    "                \"\"\",\n",
    "                tx_rows, page_size=200)\n",
    "\n",
    "        # Insert inputs\n",
    "        if not inputs_df.empty:\n",
    "            input_rows = [\n",
    "                (row.txid, row.prev_txid, row.prev_index, row.input_address, row.input_value)\n",
    "                for _, row in inputs_df.iterrows()\n",
    "            ]\n",
    "            execute_batch(cur,\n",
    "                \"\"\"\n",
    "                INSERT INTO bitcoin_inputs (txid, prev_txid, prev_index, input_address, input_value)\n",
    "                VALUES (%s, %s, %s, %s, %s)\n",
    "                \"\"\",\n",
    "                input_rows, page_size=500)\n",
    "\n",
    "        # Insert outputs\n",
    "        if not outputs_df.empty:\n",
    "            output_rows = [\n",
    "                (row.txid, row.vout_index, row.output_address, row.output_value, row.spent)\n",
    "                for _, row in outputs_df.iterrows()\n",
    "            ]\n",
    "            execute_batch(cur,\n",
    "                \"\"\"\n",
    "                INSERT INTO bitcoin_outputs (txid, vout_index, output_address, output_value, spent)\n",
    "                VALUES (%s, %s, %s, %s, %s)\n",
    "                \"\"\",\n",
    "                output_rows, page_size=500)\n",
    "\n",
    "        # Upsert addresses: we store cumulative totals coming from transform\n",
    "        if not address_df.empty:\n",
    "            addr_rows = [\n",
    "                (row.address, row.total_received, row.total_sent, row.balance, \n",
    "                 (row.first_seen.to_pydatetime() if not pd.isna(row.first_seen) else None),\n",
    "                 (row.last_seen.to_pydatetime() if not pd.isna(row.last_seen) else None))\n",
    "                for _, row in address_df.iterrows()\n",
    "            ]\n",
    "            # We'll upsert by summing totals (the transform already computed per-block deltas)\n",
    "            execute_batch(cur,\n",
    "                \"\"\"\n",
    "                INSERT INTO bitcoin_addresses (address, total_received, total_sent, balance, first_seen, last_seen)\n",
    "                VALUES (%s, %s, %s, %s, %s, %s)\n",
    "                ON CONFLICT (address)\n",
    "                DO UPDATE SET\n",
    "                    total_received = bitcoin_addresses.total_received + EXCLUDED.total_received,\n",
    "                    total_sent = bitcoin_addresses.total_sent + EXCLUDED.total_sent,\n",
    "                    balance = bitcoin_addresses.balance + EXCLUDED.balance,\n",
    "                    last_seen = GREATEST(bitcoin_addresses.last_seen, EXCLUDED.last_seen);\n",
    "                \"\"\",\n",
    "                addr_rows, page_size=200)\n",
    "\n",
    "        conn.commit()\n",
    "        logger.info(\"Load completed into raw tables.\")\n",
    "    except Exception:\n",
    "        conn.rollback()\n",
    "        logger.error(\"Load step failed\")\n",
    "        logger.error(traceback.format_exc())\n",
    "        raise\n",
    "    finally:\n",
    "        try:\n",
    "            cur.close()\n",
    "            conn.close()\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "\n",
    "# DATAMART UPDATERS\n",
    "# Note: these functions do NOT commit; commit happens in update_marts transaction wrapper\n",
    "\n",
    "def dm_update_block_stats(cur, block_height: int):\n",
    "    cur.execute(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS datamart.mart_block_stats (\n",
    "        height BIGINT PRIMARY KEY,\n",
    "        block_date DATE,\n",
    "        tx_count NUMERIC,\n",
    "        avg_fee NUMERIC,\n",
    "        total_fees NUMERIC,\n",
    "        avg_tx_size NUMERIC,\n",
    "        avg_block_size NUMERIC,\n",
    "        avg_block_weight NUMERIC\n",
    "    );\n",
    "    \"\"\")\n",
    "    cur.execute(\"\"\"\n",
    "    INSERT INTO datamart.mart_block_stats (height, block_date, tx_count, avg_fee, total_fees, avg_tx_size, avg_block_size, avg_block_weight)\n",
    "    SELECT \n",
    "        b.height,\n",
    "        b.timestamp::date,\n",
    "        COUNT(t.txid) AS tx_count,\n",
    "        COALESCE(AVG(t.fee), 0) AS avg_fee,\n",
    "        COALESCE(SUM(t.fee), 0) AS total_fees,\n",
    "        COALESCE(AVG(t.size), 0) AS avg_tx_size,\n",
    "        COALESCE(AVG(b.size), 0) AS avg_block_size,\n",
    "        COALESCE(AVG(b.weight), 0) AS avg_block_weight\n",
    "    FROM bitcoin_blocks b\n",
    "    LEFT JOIN bitcoin_transactions t ON b.block_id = t.block_id\n",
    "    WHERE b.height = %s\n",
    "    GROUP BY b.height, b.timestamp::date\n",
    "    ON CONFLICT (height) DO NOTHING;\n",
    "    \"\"\", (block_height,))\n",
    "\n",
    "def dm_update_address_activity(cur, block_height: int):\n",
    "    cur.execute(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS datamart.mart_address_activity (\n",
    "        address TEXT,\n",
    "        block_height BIGINT,\n",
    "        total_received NUMERIC,\n",
    "        total_sent NUMERIC,\n",
    "        balance NUMERIC,\n",
    "        PRIMARY KEY(address, block_height)\n",
    "    );\n",
    "    \"\"\")\n",
    "    cur.execute(\"\"\"\n",
    "    INSERT INTO datamart.mart_address_activity (address, block_height, total_received, total_sent, balance)\n",
    "    SELECT \n",
    "        a.address,\n",
    "        b.height,\n",
    "        a.total_received,\n",
    "        a.total_sent,\n",
    "        a.balance\n",
    "    FROM bitcoin_addresses a\n",
    "    JOIN bitcoin_outputs o ON a.address = o.output_address\n",
    "    JOIN bitcoin_transactions t ON o.txid = t.txid\n",
    "    JOIN bitcoin_blocks b ON t.block_id = b.block_id\n",
    "    WHERE b.height = %s\n",
    "    ON CONFLICT (address, block_height) DO NOTHING;\n",
    "    \"\"\", (block_height,))\n",
    "\n",
    "def dm_update_tx_patterns(cur, block_height: int):\n",
    "    cur.execute(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS datamart.mart_tx_patterns (\n",
    "        block_height BIGINT,\n",
    "        txid TEXT PRIMARY KEY,\n",
    "        input_count NUMERIC,\n",
    "        output_count NUMERIC,\n",
    "        total_input_value NUMERIC,\n",
    "        total_output_value NUMERIC\n",
    "    );\n",
    "    \"\"\")\n",
    "    cur.execute(\"\"\"\n",
    "    INSERT INTO datamart.mart_tx_patterns (block_height, txid, input_count, output_count, total_input_value, total_output_value)\n",
    "    SELECT\n",
    "        b.height,\n",
    "        t.txid,\n",
    "        COALESCE(COUNT(i.id),0) AS input_count,\n",
    "        COALESCE(COUNT(o.id),0) AS output_count,\n",
    "        COALESCE(SUM(i.input_value), 0) AS total_input_value,\n",
    "        COALESCE(SUM(o.output_value), 0) AS total_output_value\n",
    "    FROM bitcoin_transactions t\n",
    "    LEFT JOIN bitcoin_inputs i ON t.txid = i.txid\n",
    "    LEFT JOIN bitcoin_outputs o ON t.txid = o.txid\n",
    "    JOIN bitcoin_blocks b ON t.block_id = b.block_id\n",
    "    WHERE b.height = %s\n",
    "    GROUP BY b.height, t.txid\n",
    "    ON CONFLICT (txid) DO NOTHING;\n",
    "    \"\"\", (block_height,))\n",
    "\n",
    "\n",
    "\n",
    "def dm_update_daily_tx_volume(cur, block_height: int):\n",
    "    cur.execute(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS datamart.mart_daily_tx_volume (\n",
    "        block_date DATE PRIMARY KEY,\n",
    "        tx_count NUMERIC,\n",
    "        total_fees NUMERIC,\n",
    "        total_output_value NUMERIC\n",
    "    );\n",
    "    \"\"\")\n",
    "    cur.execute(\"\"\"\n",
    "    INSERT INTO datamart.mart_daily_tx_volume (block_date, tx_count, total_fees, total_output_value)\n",
    "    SELECT \n",
    "        b.timestamp::date AS block_date,\n",
    "        COUNT(t.txid) AS tx_count,\n",
    "        COALESCE(SUM(t.fee), 0) AS total_fees,\n",
    "        COALESCE(SUM(o.output_value), 0) AS total_output_value\n",
    "    FROM bitcoin_blocks b\n",
    "    LEFT JOIN bitcoin_transactions t ON b.block_id = t.block_id\n",
    "    LEFT JOIN bitcoin_outputs o ON t.txid = o.txid\n",
    "    WHERE b.height = %s\n",
    "    GROUP BY b.timestamp::date\n",
    "    ON CONFLICT (block_date) DO UPDATE \n",
    "    SET tx_count = EXCLUDED.tx_count,\n",
    "        total_fees = EXCLUDED.total_fees,\n",
    "        total_output_value = EXCLUDED.total_output_value;\n",
    "    \"\"\", (block_height,))\n",
    "\n",
    "\n",
    "def update_marts(conn, block_height: int):\n",
    "    \"\"\"\n",
    "    Wrap all mart updates in a single transaction so they commit atomically.\n",
    "    \"\"\"\n",
    "    cur = conn.cursor()\n",
    "    try:\n",
    "        cur.execute(\"SET LOCAL synchronous_commit = OFF;\")  # optional perf tweak\n",
    "        dm_update_block_stats(cur, block_height)\n",
    "        dm_update_tx_patterns(cur, block_height)\n",
    "        dm_update_address_activity(cur, block_height)\n",
    "        dm_update_daily_tx_volume(cur, block_height)\n",
    "        conn.commit()\n",
    "        logger.info(f\"Updated data marts for block {block_height}\")\n",
    "    except Exception:\n",
    "        conn.rollback()\n",
    "        logger.error(f\"Failed updating datamarts for block {block_height}\")\n",
    "        logger.error(traceback.format_exc())\n",
    "        raise\n",
    "    finally:\n",
    "        cur.close()\n",
    "\n",
    "\n",
    "# STREAMING WITH CHECKPOINT + GRACEFUL SHUTDOWN\n",
    "\n",
    "def stream_blocks(poll_interval=POLL_INTERVAL):\n",
    "    ensure_state_table()\n",
    "    last_height = get_last_processed_height()\n",
    "\n",
    "    if last_height:\n",
    "        logger.info(f\"Resuming from checkpoint block {last_height}\")\n",
    "    else:\n",
    "        if START_HEIGHT:\n",
    "            last_height = START_HEIGHT\n",
    "            logger.info(f\"Starting from configured block height {START_HEIGHT}\")\n",
    "        elif START_DATE:\n",
    "            last_height = get_height_from_date(START_DATE)\n",
    "            logger.info(f\"Starting from date {START_DATE}, block height {last_height}\")\n",
    "        else:\n",
    "            last_height = get_latest_height()\n",
    "            logger.info(f\"Starting from tip block {last_height}\")\n",
    "        update_last_processed_height(last_height)\n",
    "\n",
    "    try:\n",
    "        while True:\n",
    "            try:\n",
    "                tip = get_latest_height()\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Failed to fetch latest height: {e}\")\n",
    "                time.sleep(poll_interval)\n",
    "                continue\n",
    "\n",
    "            if last_height < tip:\n",
    "                for h in range(last_height + 1, tip + 1):\n",
    "                    logger.info(f\"Processing block {h}\")\n",
    "                    try:\n",
    "                        block_data, detailed_txs = extract_block(h)\n",
    "                        block_df, tx_df, inputs_df, outputs_df, address_df = transform_block(block_data, detailed_txs)\n",
    "                        # load raw\n",
    "                        load_to_postgres(block_df, tx_df, inputs_df, outputs_df, address_df)\n",
    "                        # update marts\n",
    "                        conn = get_connection()\n",
    "                        try:\n",
    "                            update_marts(conn, h)\n",
    "                        finally:\n",
    "                            conn.close()\n",
    "                        # update checkpoint\n",
    "                        update_last_processed_height(h)\n",
    "                        last_height = h\n",
    "                    except Exception:\n",
    "                        logger.error(f\"Failed to process block {h}, continuing to next block\")\n",
    "                        logger.error(traceback.format_exc())\n",
    "                        \n",
    "            else:\n",
    "                logger.info(\"No new block yet\")\n",
    "            time.sleep(poll_interval)\n",
    "    except KeyboardInterrupt:\n",
    "        logger.info(\"Received KeyboardInterrupt - stopping stream gracefully\")\n",
    "    except Exception:\n",
    "        logger.critical(\"ETL pipeline crashed unexpectedly\")\n",
    "        logger.critical(traceback.format_exc())\n",
    "        raise\n",
    "\n",
    "\n",
    "# MAIN\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        stream_blocks(poll_interval=POLL_INTERVAL)\n",
    "    except Exception as e:\n",
    "        logger.critical(f\"ETL pipeline crashed: {e}\")\n",
    "        logger.critical(traceback.format_exc())\n",
    "        raise\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
